import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')

# 检查CUDA可用性
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用设备: {device}")


to_onehot = lambda x, length=4: [int(i == x) for i in range(length)]

specific_diseases = ["Stroke_status",  "Depression_status", "AD_status"]


def calculate_label(row, key):
    # 条件1: key_pd 同时 has_gi 为3
    if row[key] and row['has_gi'] :
        return 3
    # 条件2: 只有key_pd 没有has_gi为2
    elif row[key] and not row['has_gi']:
        return 2
    # 条件3: has_gi 或者 has_neuro 不包含has_pd 为1
    elif row['has_gi'] or  row['has_neuro']:
        return 1
    # 其他情况
    else:
        return 0



def load_data(file_path):
    print("=== 步骤1: 数据准备和预处理 ===")
    jiqi = pd.read_csv(file_path)

    # 定义疾病变量
    gi_diseases = ["colorectal_status", "liver_status", "GERD_status", "IBD_status", 
                "IBS_status", "ALD_status", "NAFLD_status"]
    neuro_diseases = ["Stroke_status", "Anxiety_status", "Bipolar.affective.disorder_status", 
                    "Depression_status", "ACD_status", "AD_status", "VAD_status", 
                    "PD_status", "OACPD_status"]

    #specific_diseases = ["Stroke_status",  "Depression_status", "AD_status"]


    # 过滤存在的列
    gi_diseases = [col for col in gi_diseases if col in jiqi.columns]
    neuro_diseases = [col for col in neuro_diseases if col in jiqi.columns]


    to_onehot_gi = lambda x:  np.array([0 for i in range(len(gi_diseases))])
    to_onehot_gi_2 = lambda x, idx=0:  np.array([int(i==idx) * x for i in range(len(gi_diseases))])
    jiqi['gi_label'] = jiqi[gi_diseases[0]].apply(to_onehot_gi)
    for idx, gi_diseas in enumerate(gi_diseases):
        jiqi['gi_label'] += jiqi[gi_diseas].apply(to_onehot_gi_2, idx=idx)

    to_onehot_neuro = lambda x:  np.array([0 for i in range(len(neuro_diseases))])
    to_onehot_neuro_2 = lambda x, idx=0:  np.array([int(i==idx) * x for i in range(len(neuro_diseases))])
    jiqi['neuro_label'] = jiqi[neuro_diseases[0]].apply(to_onehot_neuro)
    for idx, neuro_diseas in enumerate(neuro_diseases):
        jiqi['neuro_label'] += jiqi[neuro_diseas].apply(to_onehot_neuro_2, idx=idx)

    # 创建共病标签
    jiqi['has_gi'] = jiqi[gi_diseases].apply(lambda x: any(x == 1), axis=1)
    jiqi['has_neuro'] = jiqi[neuro_diseases].apply(lambda x: any(x == 1), axis=1)
    jiqi['comorbidity_status'] = (jiqi['has_gi'] & jiqi['has_neuro']).astype(int)

    jiqi['comorbidity_status_multilabel'] = (jiqi['has_gi']).astype(int) + (jiqi['has_neuro']).astype(int) *2
    jiqi['comorbidity_status_multilabel'] = jiqi['comorbidity_status_multilabel'].apply(to_onehot)
    #jiqi['comorbidity_status_multilabel'] = jiqi['comorbidity_status'].apply(to_onehot,length=2)

    for key in specific_diseases:
        key_pd = 'has_' + key    
        jiqi[key_pd] = jiqi[key].apply(lambda x: int(x == 1))
        jiqi[key_pd + '_multilabel'] = jiqi.apply(calculate_label,axis=1, key=key_pd)
        jiqi[key_pd + '_multilabel'] = jiqi[key_pd + '_multilabel'].apply(to_onehot,length=4)
        #print(jiqi[key_pd + '_multilabel'].value_counts())


    for idx, val in enumerate(jiqi['gi_label'].mean() *100):
        item = gi_diseases[idx]
        print(f"gi_diseases {item} 病患病率: {val:.2f}%")

    for idx, val in enumerate(jiqi['neuro_label'].mean() *100):
        item = neuro_diseases[idx]
        print(f"神经系统{item}病患病率: {val:.2f}%")

    print(f"共病样本数: {jiqi['comorbidity_status'].sum()}")
    print(f"共病率: {jiqi['comorbidity_status'].mean() * 100:.2f}%")

    # 提取蛋白质特征
    protein_cols = jiqi.loc[:, 'ADA2':'WARS'].columns.tolist()
    print(f"蛋白质特征数量: {len(protein_cols)}")

    # 处理协变量
    covariates = ['age', 'TDI', 'BMI', 'Physical_activity']
    available_covariates = [col for col in covariates if col in jiqi.columns]

    # 处理Physical_activity列中的文本值
    if 'Physical_activity' in jiqi.columns:
        # 将文本值映射为数值
        activity_map = {'moderate': 1}
        jiqi['Physical_activity'] = jiqi['Physical_activity'].map(activity_map).fillna(jiqi['Physical_activity'])
        # 确保该列为数值类型
        jiqi['Physical_activity'] = pd.to_numeric(jiqi['Physical_activity'], errors='coerce').fillna(0)

    # 处理分类变量
    if 'sex' in jiqi.columns:
        jiqi['sex_numeric'] = jiqi['sex'].map({'male': 1, 'female': 0}).fillna(0)
    else:
        jiqi['sex_numeric'] = 0

    categorical_cols = ['sex_numeric']
    if 'ethnic' in jiqi.columns:
        jiqi['ethnic_white'] = (jiqi['ethnic'] == 'White').astype(int)
        categorical_cols.append('ethnic_white')

    # 合并所有特征
    all_features = protein_cols + available_covariates + categorical_cols
    all_features = [col for col in all_features if col in jiqi.columns]

    X = jiqi[all_features].fillna(0).astype(float)
    #y = jiqi['comorbidity_status'].values

    # jiqi['x'] = X
    # jiqi['y'] = y

    print(f"总特征数: {X.shape[1]}")

    label_cols = ['gi_label', 'neuro_label', 'comorbidity_status_multilabel']
    for key in specific_diseases:
        key_pd = 'has_' + key
        label_cols.append(key_pd + '_multilabel')
    return {
        "original_pd": jiqi,
        "X": X,
        "Y": jiqi[label_cols],
        "protein_cols": protein_cols
        #"Y": jiqi[['gi_label', 'neuro_label', 'comorbidity_status_multilabel']]
    }

# ========================== 神经网络模型定义 ==========================
class PreTrainedProteinModel(nn.Module):
    def __init__(self, n_proteins=200, n_gi=7, n_neuro=9, dropout_ratio=0.3):
        super(PreTrainedProteinModel, self).__init__()
        self.n_gi = n_gi
        self.n_neuro = n_neuro
        self.n_proteins = n_proteins
        self.feature_dim = 64
        
        # 共享蛋白质特征提取网络
        self.protein_encoder_gi = nn.Sequential(
            nn.Linear(n_proteins, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_ratio),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_ratio)
        )

        self.protein_encoder_neuro = nn.Sequential(
            nn.Linear(n_proteins, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_ratio),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_ratio)
        )

        
        # 多任务输出头
        self.mutil_task_heads_gi = nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_gi)
            ) 
        

        self.mutil_task_heads_neuro =  nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_neuro)
            ) 
        

        
    def forward(self, x):
        gi_feature = self.protein_encoder_gi(x)
        neuro_feature = self.protein_encoder_neuro(x)
        gi_pred = self.mutil_task_heads_gi(gi_feature)
        neuro_pred = self.mutil_task_heads_neuro(neuro_feature)

        return {
            "gi_feature": gi_feature,
            "neuro_feature": neuro_feature,
            "gi_pred": gi_pred,
            "neuro_pred": neuro_pred
        }


class SpecificityProteinNet(nn.Module):
    def __init__(self, PreTrainModel, n_proteins=200, n_combin=4, dropout_ratio=0.3):
        super(SpecificityProteinNet, self).__init__()
        self.PreTrainModel = PreTrainModel
        self.n_combin = n_combin
        self.n_proteins = n_proteins
        self.dropout_ratio = dropout_ratio
        self.feature_dim = PreTrainModel.feature_dim

        self.input_attn_dim = 16 
        self.num_heads = 4
        self.head_dim = self.input_attn_dim * self.num_heads
        self.layer_norm = nn.LayerNorm(self.feature_dim)

        #cross attention module
   
        out_dim = n_proteins
        self.query = nn.Linear(self.feature_dim, self.head_dim)
        self.key = nn.Linear(self.feature_dim, self.head_dim)
        self.value = nn.Linear(self.feature_dim, self.feature_dim)

        # self.query = nn.Linear(self.input_attn_dim, self.input_attn_dim)
        # self.key = nn.Linear(self.input_attn_dim, self.input_attn_dim)
        # self.value = nn.Linear(self.input_attn_dim, self.input_attn_dim)


        self.out_proj = nn.Linear(self.feature_dim, out_dim)


        self.encoder = nn.Sequential(
            nn.Linear(n_proteins, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_ratio),
            nn.Linear(128, self.feature_dim ),
            nn.BatchNorm1d(self.feature_dim ),
            nn.ReLU(),
            nn.Dropout(dropout_ratio)
        )

        # 任务特定头
        self.task_specific_heads = nn.Sequential(
                nn.Linear(self.feature_dim*3, 32),
                nn.BatchNorm1d(32),
                nn.ReLU(),
                nn.Dropout(dropout_ratio),
                nn.Linear(32, n_combin),
                nn.Softmax(dim=-1)
                #nn.ReLU()
            ) 
        

    def forward(self, x):
        with torch.no_grad():
            pretrain_out = self.PreTrainModel(x)
            gi_feature = pretrain_out["gi_feature"]
            neuro_feature = pretrain_out["neuro_feature"]


        q = self.query(gi_feature).reshape(-1, self.num_heads, self.input_attn_dim)  # [batch_size, seq_len, input_dim]
        k = self.key(neuro_feature).reshape(-1, self.num_heads, self.input_attn_dim)  # [batch_size, context_len, input_dim]
        v = self.value(neuro_feature).reshape(-1, self.num_heads, self.feature_dim//self.num_heads)  # [batch_size, context_len, input_dim]


        # q = self.query(gi_feature.reshape(-1, self.feature_dim//self.input_attn_dim,
        #             self.input_attn_dim))  # [batch_size, seq_len, input_dim]
        # k = self.key(neuro_feature.reshape(-1,self.feature_dim//self.input_attn_dim,
        #             self.input_attn_dim))  # [batch_size, context_len, input_dim]
        # v = self.value(neuro_feature.reshape(-1, self.feature_dim//self.input_attn_dim,
        #         self.input_attn_dim))  # [batch_size, context_len, input_dim]


        
        # 计算注意力权重
        #attn_weights = torch.bmm(q, k.transpose(1, 2))  # [batch_size, seq_len, context_len]
        attn_weights = torch.bmm(q, k.transpose(1, 2)) / (self.input_attn_dim ** 0.5)  # [batch_size, seq_len, context_len]
        attn_weights = F.softmax(attn_weights, dim=-1)
        
        # 应用注意力
        output = torch.bmm(attn_weights, v)  # [batch_size, seq_len, input_dim]
        output = output.reshape(-1, self.feature_dim)  # [batch_size, seq_len*input_dim]

        output = F.dropout(output, self.dropout_ratio, training=self.training)

        # 投影到n-dim
        #output = self.layer_norm(output)
        mask = self.out_proj(self.layer_norm(output))  # [batch_size, seq_len, n_dim]
        mask =  torch.sigmoid(mask)

        x  = x * mask
        #x  = x * mask + x
        x = self.encoder(x)
        x = torch.cat([x, gi_feature, neuro_feature], dim=-1)

        x = F.dropout(x, self.dropout_ratio, training=self.training)

        x = self.task_specific_heads(x)
        return {
            "combination_predictions": x,
            "attention_weights": attn_weights,
            "protein_importance": mask
        }








# ========================== 数据准备函数 ==========================
def prepare_full_protein_multitask_data(jiqi, protein_cols, gi_diseases, neuro_diseases):
    print("=== 准备全蛋白质多任务数据 ===")
    
    # 获取所有可用蛋白质
    available_proteins = [col for col in protein_cols if col in jiqi.columns]
    print(f"总蛋白质数量: {len(available_proteins)}")
    
    # 提取全蛋白质矩阵
    X_all_proteins = jiqi[available_proteins].copy()
    
    # 数据清理
    for col in X_all_proteins.columns:
        col_data = X_all_proteins[col]
        
        # 确保数值型
        if not np.issubdtype(col_data.dtype, np.number):
            X_all_proteins[col] = pd.to_numeric(col_data, errors='coerce')
        
        # 处理NA值：用中位数填充
        if X_all_proteins[col].isna().any():
            median_val = X_all_proteins[col].median()
            if pd.isna(median_val):
                median_val = 0
            X_all_proteins[col].fillna(median_val, inplace=True)
    
    # 标准化所有蛋白质
    scaler = StandardScaler()
    X_proteins_scaled = scaler.fit_transform(X_all_proteins)
    X_proteins_scaled = np.nan_to_num(X_proteins_scaled, nan=0, posinf=0, neginf=0)
    
    print(f"✓ 蛋白质特征矩阵: {X_proteins_scaled.shape}")
    
    # 准备所有疾病组合标签
    gi_diseases_available = [d for d in gi_diseases if d in jiqi.columns]
    neuro_diseases_available = [d for d in neuro_diseases if d in jiqi.columns]
    
    print(f"GI疾病: {len(gi_diseases_available)} 种")
    print(f"Neuro疾病: {len(neuro_diseases_available)} 种")
    
    # 创建所有两两组合
    n_combinations = len(gi_diseases_available) * len(neuro_diseases_available)
    combination_matrix = np.zeros((jiqi.shape[0], n_combinations))
    combination_info = {
        "gi_disease": [],
        "neuro_disease": [],
        "combination_name": [],
        "positive_cases": []
    }
    
    col_idx = 0
    for i in range(len(gi_diseases_available)):
        for j in range(len(neuro_diseases_available)):
            gi_disease = gi_diseases_available[i]
            neuro_disease = neuro_diseases_available[j]
            
            # 检查同时患病
            has_both = (jiqi[gi_disease] == 1) & (jiqi[neuro_disease] == 1)
            has_both.fillna(False, inplace=True)
            
            combination_matrix[:, col_idx] = has_both.astype(int)
            
            combination_info["gi_disease"].append(gi_disease)
            combination_info["neuro_disease"].append(neuro_disease)
            combination_info["combination_name"].append(f"{gi_disease}+{neuro_disease}")
            combination_info["positive_cases"].append(has_both.sum())
            
            col_idx += 1
    
    # 转换为DataFrame
    combination_info_df = pd.DataFrame(combination_info)
    
    # 显示组合统计
    combination_info_df = combination_info_df.sort_values(by="positive_cases", ascending=False)
    print("\n前10种最常见组合:")
    for i in range(min(10, len(combination_info_df))):
        row = combination_info_df.iloc[i]
        print(f"{i+1}. {row['combination_name']}: {row['positive_cases']}例")
    
    return {
        "X": X_proteins_scaled,
        "Y": combination_matrix,
        "combination_info": combination_info_df,
        "protein_names": available_proteins,
        "n_proteins": len(available_proteins),
        "n_combinations": n_combinations
    }

# ========================== 数据集类 ==========================
class ProteinDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.tensor(X, dtype=torch.float32)
        if type(Y) == pd.DataFrame: 
            self.Y_gi = torch.tensor(np.vstack(Y['gi_label'].values).astype(np.float32), dtype=torch.float32)
            self.Y_neuro = torch.tensor(np.vstack(Y['neuro_label'].values).astype(np.float32), dtype=torch.float32)
            self.Y_mutil = torch.tensor(np.vstack(Y['comorbidity_status_multilabel'].values).astype(np.float32), dtype=torch.float32)

    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y_gi[idx], self.Y_neuro[idx], self.Y_mutil[idx]

class ProteinDatasetv2(Dataset):
    def __init__(self, X, Y, key):
        self.X = torch.tensor(X, dtype=torch.float32)
        if type(Y) == pd.DataFrame and key in Y.columns: 
            self.Y = torch.tensor(np.vstack(Y[key].values).astype(np.float32), dtype=torch.float32)

    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y[idx]



def train_step_specific(model, train_loader, epochs, lr, X_test, Y_test, key, alpha):


    def eval_model(model, X_test, Y_test, epoch, best_auc, key):
        print("\n进行测试集预测...")
        model.eval()
        pred_test = []
        heatmap = []

        with torch.no_grad():
            # 分批处理测试数据避免内存问题
            test_batch_size = 100
            n_test = X_test.shape[0]
            
            for i in range(0, n_test, test_batch_size):
                end_idx = min(i + test_batch_size, n_test)
                X_batch = X_test[i:end_idx]
                X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)
                
                outputs = model(X_batch_tensor)
                
                pred_test.append(outputs["combination_predictions"].cpu().numpy())
                heatmap.append(outputs["protein_importance"].cpu().numpy())
                #protein_importance.append(outputs["protein_importance"].cpu().numpy())

        pred_test = np.vstack(pred_test)
        protein_importance = np.vstack(heatmap)


        #true_labels = np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1)
        true_labels = Y_test.argmax(axis=1)
        pred_labels = pred_test.argmax(axis=1)

        # 计算每个类别的准确率
        for i in range(4):
            mask = true_labels == i
            if mask.any():
                acc = (pred_labels[mask] == i).mean()
            else:
                acc = 0.0
            print(f"Class_{i} ACC (epoch {epoch}): {acc:.4f}")

        # 计算整体准确率
        overall_acc = (pred_labels == true_labels).mean()
        print(f"Overall ACC (epoch {epoch}): {overall_acc:.4f}")


        from sklearn.metrics import roc_auc_score
        #auc_sep = roc_auc_score(np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1).astype(np.int32),
   #         torch.tensor(pred_test).numpy(), multi_class='ovr', average=None)
        auc_sep = roc_auc_score(Y_test.argmax(axis=1).astype(np.int32), torch.tensor(pred_test).numpy(), multi_class='ovr', average=None)
        for i in range(4):
            print(f"Class_{i} AUC (epoch {epoch}): {auc_sep[i]:.4f}")

        # 计算整体AUC
        # auc = roc_auc_score(np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1).astype(np.int32),
        #     torch.tensor(pred_test).numpy(), multi_class='ovr')
        auc = roc_auc_score(Y_test.argmax(axis=1).astype(np.int32), torch.tensor(pred_test).numpy(), multi_class='ovr')
        print(f"AUC_{epoch}: {auc:.4f}")

        if auc_sep[3] > best_auc:
            best_auc = auc_sep[3]
            #save the model
            torch.save(model.state_dict(), 'model_specific_' + key + '_' + str(epoch) + '_' + str(best_auc) + '.pth')

        return best_auc

    # 计算参数数量
    total_params = sum(p.numel() for p in model.parameters())
    print(f"模型参数数量: {total_params}")
    
    # 优化器和学习率调度器
    from torch.optim.lr_scheduler import CosineAnnealingLR
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader), eta_min=0)

    # 训练循环
    train_losses = []
    print("\n开始训练...")


    class FocalLoss(nn.Module):
        def __init__(self, alpha=1, gamma=2, reduction='mean'):
            super(FocalLoss, self).__init__()
            self.alpha = alpha
            self.gamma = gamma
            self.reduction = reduction

        def forward(self, inputs, targets):
            # 计算交叉熵损失
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
            
            # 获取预测概率
            pt = torch.exp(-ce_loss)
            
            # 计算Focal Loss
            focal_loss = (1-pt)**self.gamma * ce_loss

            if self.alpha is not None:
                if isinstance(self.alpha, (float, int)):  # 如果是标量，直接广播
                    alpha_t = torch.full_like(targets, self.alpha, dtype=torch.float32)
                else:  # 如果是张量 [num_classes]，按 targets 索引选取 alpha
                    alpha_t = self.alpha[targets.argmax(1).cpu().numpy()].to(focal_loss.device)  # [batch_size]
                focal_loss = alpha_t * focal_loss  # 按样本类别加权

            if self.reduction == 'mean':
                return focal_loss.mean()
            elif self.reduction == 'sum':
                return focal_loss.sum()
            else:
                return focal_loss

            

        def auc_loss_for_class_3(self, inputs, targets):
            """专门优化共病类(Class 3)的AUC"""
            # 将四分类转换为二分类：Class 3 vs Others
            class_3_prob = F.softmax(inputs, dim=1)[:, 3]  # Class 3的概率
            is_class_3 = (targets.argmax(dim=1) == 3).float()  # 是否为Class 3
            
            if is_class_3.sum() == 0 or (1 - is_class_3).sum() == 0:
                return torch.tensor(0.0, device=inputs.device)
            
            # 计算AUC损失
            pos_indices = (is_class_3 == 1).nonzero().squeeze()
            neg_indices = (is_class_3 == 0).nonzero().squeeze()
            
            if pos_indices.numel() == 0 or neg_indices.numel() == 0:
                return torch.tensor(0.0, device=inputs.device)
                
            pos_scores = class_3_prob[pos_indices]
            neg_scores = class_3_prob[neg_indices]
            
            # 计算所有正负样本对的差值
            pos_scores = pos_scores.unsqueeze(1)  # [n_pos, 1]
            neg_scores = neg_scores.unsqueeze(0)  # [1, n_neg]
            
            # AUC损失：最大化 P(pos_score > neg_score)
            diff = pos_scores - neg_scores  # [n_pos, n_neg]
            auc_loss = torch.mean(torch.sigmoid(-diff))
            
            return auc_loss


    best_auc = 0 
    # alpha = torch.tensor([0.05, 0.05, 0.3, 0.6])  # 总和为1（可选）
    #alpha = torch.tensor([0.08, 0.25, 0.2, 0.47])  # 总和为1（可选）
    alpha = torch.tensor(alpha)
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        n_batches = 0
        
        for batch_x, batch_y  in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            #batch_y_gi = batch_y_gi.to(device)
            #batch_y_neuro = batch_y_neuro.to(device)

            optimizer.zero_grad()
            
            # 前向传播
            outputs = model(batch_x)            
            #"gi_pred": gi_pred,
            #"neuro_pred": neuro_pred

            # 或 alpha = torch.tensor([99, 1])   # 总和为100（同样有效）
            focal_loss = FocalLoss(alpha=alpha, gamma=2)
            loss = focal_loss(outputs['combination_predictions'], batch_y)

            #auc_loss = focal_loss.auc_loss_for_class_3(outputs['combination_predictions'], batch_y)
            #loss = 0.7 * loss + 0.3 * auc_loss 


            # 计算损失
            #loss = F.cross_entropy(outputs['combination_predictions'],batch_y)
            
            # 检查损失是否有效
            if not torch.isnan(loss) and not torch.isinf(loss):
                # 反向传播
                loss.backward()
                scheduler.step()
                #print("LR =", scheduler.get_last_lr())
                
                # 梯度裁剪
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                n_batches += 1
            else:
                print("警告：发现无效损失值，跳过此批次")
        
        if n_batches > 0:

            avg_loss = epoch_loss / n_batches
            train_losses.append(avg_loss)

            
            if (epoch + 1) % 1 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}")
                best_auc = eval_model(model, X_test, Y_test, epoch+1, best_auc, key)
        else:
            print(f"Epoch {epoch+1}/{epochs} - 无有效批次")
            break
    
    # 测试集预测
    
    # heatmap_draw(protein_importance, np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1).astype(np.int32))

    # visulazation the attention mask
    return  model



def heatmap_draw(data, label):
    import numpy as np
    import matplotlib
    matplotlib.use('Agg')  # 使用非交互式后端
    import seaborn as sns
    import matplotlib.pyplot as plt

        # 将数据和标签转换为DataFrame
    df = pd.DataFrame(data)
    df['Label'] = label  # 添加标签列
    
    # 按标签分组并计算每个特征的均值
    grouped = df.groupby('Label').mean()
    
    # 创建示例数据
    #data = np.random.rand(10, 12)  # 10行12列的随机数据

    # 创建热力图
    plt.figure(figsize=(10, 8))
    sns.heatmap(grouped, cmap="YlGnBu", fmt=".2f")


    plt.title("基本热力图示例")
    plt.savefig("heatmap.png", dpi=300, bbox_inches="tight")

# ========================== 训练函数 ==========================
def train_full_protein_multitask_model(data, pretrain_path= "", epochs=10, lr=0.001, batch_size=32):
    print("=== 训练多任务深度神经网络 ===")

    
    # 数据分割
    X_train, X_test, Y_train, Y_test = train_test_split(
        data["X"].values.astype(np.float32), data["Y"], test_size=0.2, random_state=42
    )

    print(f"训练集: {X_train.shape[0]} 样本")
    print(f"测试集: {X_test.shape[0]} 样本")
    print(f"蛋白质特征: {X_train.shape[1]} 个")
    print(f"疾病组合: {Y_train.shape[1]} 个")

    batch_size = max(batch_size, 16)

    # 创建数据集和数据加载器
    train_dataset = ProteinDataset(X_train, Y_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # 初始化模型
    model = PreTrainedProteinModel(
         n_proteins=X_train.shape[1],
         n_gi=Y_train['gi_label'][0].shape[0], 
         n_neuro=Y_train['neuro_label'][0].shape[0],
         dropout_ratio=0.5
    ).to(device)

    if os.path.exists(pretrain_path):
        print(f"加载预训练模型: {pretrain_path}")
        model.load_state_dict(torch.load(pretrain_path, map_location=device))
    else:
        print("未找到预训练模型，将从头开始训练。")
        def eval_multi_task_model(model, X_test, Y_test, epoch, best_auc=0.0):
                
            # 测试集预测
            print("\n进行测试集预测...")
            model.eval()
            gi_pred_test = []
            neuro_pred_test = []

            
            with torch.no_grad():
                # 分批处理测试数据避免内存问题
                test_batch_size = 100
                n_test = X_test.shape[0]
                
                for i in range(0, n_test, test_batch_size):
                    end_idx = min(i + test_batch_size, n_test)
                    X_batch = X_test[i:end_idx]
                    X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)
                    
                    outputs = model(X_batch_tensor)
                    
                    gi_pred_test.append(outputs["gi_pred"].cpu().numpy())
                    neuro_pred_test.append(outputs["neuro_pred"].cpu().numpy())
                    #protein_importance.append(outputs["protein_importance"].cpu().numpy())
            
            gi_pred_test = np.vstack(gi_pred_test)
            neuro_pred_test = np.vstack(neuro_pred_test)
            #protein_importance = np.vstack(protein_importance)

            ## count the acc of the predictions mutil label 
            def multi_label_auc(y_true, y_pred):

                from sklearn.metrics import roc_auc_score
                """
                计算多标签AUC的函数
                :param y_true: 真实标签，形状为[N, num_classes]
                :param y_pred: 预测标签，形状为[N, num_classes]
                :return: 多标签AUC
                """
                # 将标签转换为numpy数组
                #y_true = y_true.cpu().numpy()
                #y_pred = y_pred.cpu().numpy()

                # 初始化多标签AUC值
                total_auc = 0.

                # 计算每个标签的AUC值，并对所有标签的AUC值求平均
                for i in range(y_true.shape[1]):
                    try:
                        auc = roc_auc_score(y_true[:, i], y_pred[:, i])
                        print(f"Label {i} AUC: {auc:.4f}")
                    except ValueError:
                        auc = 0.5  # 如果标签中只有一个类别，则返回0.5
                    total_auc += auc

                multi_auc = total_auc / y_true.shape[1]

                return multi_auc

            
            gi_auc = multi_label_auc(np.vstack(Y_test['gi_label'].values).astype(np.float32), gi_pred_test)
            neuro_auc = multi_label_auc(np.vstack(Y_test['neuro_label'].values).astype(np.float32), neuro_pred_test)

            #gi_acc = (gi_pred_test.argmax(axis=1) == Y_test['gi_label'].argmax(axis=1)).mean()
            #neuro_acc = (neuro_pred_test.argmax(axis=1) == Y_test['neuro_label'].argmax(axis=1)).mean()
            print(f"GI AUC: {gi_auc:.4f}")
            print(f"Neuro AUC: {neuro_auc:.4f}")
            if gi_auc + neuro_auc > best_auc:
                best_auc = gi_auc + neuro_auc
                print(f"保存模型: epoch_{epoch}_auc_{best_auc:.4f}.pth")
                torch.save(model.state_dict(), f"epoch_{epoch}_auc_{best_auc:.4f}.pth")

            return best_auc
    
        
        # 计算参数数量
        total_params = sum(p.numel() for p in model.parameters())
        print(f"模型参数数量: {total_params}")
        
        # 优化器和学习率调度器
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
        #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
            
            # 损失函数
        def multilabel_crossentropy(output,label):
            """
            多标签分类的交叉熵
            说明：label和output的shape一致，label的元素非0即1，
                1表示对应的类为目标类，0表示对应的类为非目标类。
            警告：请保证output的值域是全体实数，换言之一般情况下output
                不用加激活函数，尤其是不能加sigmoid或者softmax！预测
                阶段则输出output大于0的类。如有疑问，请仔细阅读并理解
                本文。
            :param output: [B,C]
            :param label:  [B,C]
            :return:
            """
            output = (1-2*label)*output
        
            #得分变为负1e12
            output_neg = output - label* 1e12
            output_pos = output-(1-label)* 1e12
        
            zeros = torch.zeros_like(output[:,:1])
        
            # [B, C + 1]
            output_neg = torch.cat([output_neg,zeros],dim=1)
            # [B, C + 1]
            output_pos = torch.cat([output_pos,zeros],dim=1)
        
            
            loss_pos = torch.logsumexp(output_pos,dim=1)
            loss_neg = torch.logsumexp(output_neg,dim=1)
            loss = (loss_neg + loss_pos).sum()
        
            return loss
        
        # 训练循环
        train_losses = []
        print("\n开始训练...")
        
        best_auc = 0.0
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            n_batches = 0
            
            for batch_x, batch_y_gi, batch_y_neuro, _  in train_loader:
                batch_x = batch_x.to(device)
                batch_y_gi = batch_y_gi.to(device)
                batch_y_neuro = batch_y_neuro.to(device)

                optimizer.zero_grad()
                
                # 前向传播
                outputs = model(batch_x)            
                #"gi_pred": gi_pred,
                #"neuro_pred": neuro_pred

                # 计算损失
                loss = multilabel_crossentropy(outputs["gi_pred"], batch_y_gi)
                loss += multilabel_crossentropy(outputs["neuro_pred"], batch_y_neuro)
                
                # 检查损失是否有效
                if not torch.isnan(loss) and not torch.isinf(loss):
                    # 反向传播
                    loss.backward()
                    
                    # 梯度裁剪
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    n_batches += 1
                else:
                    print("警告：发现无效损失值，跳过此批次")
            
            if n_batches > 0:
                scheduler.step()
                print("LR =", scheduler.get_last_lr())
                avg_loss = epoch_loss / n_batches / batch_size
                train_losses.append(avg_loss)
                
                if (epoch + 1) % 1 == 0:
                    print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}")
                    best_auc = eval_multi_task_model(model, X_test, Y_test, epoch+1, best_auc)
            else:
                print(f"Epoch {epoch+1}/{epochs} - 无有效批次")
                break

        #save the model 
        torch.save(model.state_dict(), 'model_pretrain.pth')

    #PreTraining model Traning finished 

    # training specific disease model

    print("\n开始训练疾病模型...")

    return_res = {}
    for key in specific_diseases: 
        print(f"训练特定疾病模型: {key}")
        dataset_key = 'has_' + key + '_multilabel'

    # 初始化模型
        model_specific = SpecificityProteinNet(
            PreTrainModel= model,
            n_proteins=X_train.shape[1],
            n_combin=len(Y_train['comorbidity_status_multilabel'][0]),
            dropout_ratio=0.5
        ).to(device)

        if os.path.exists('/data/ye/model_specific_' + key + '.pth'):
            model_specific.load_state_dict(torch.load('/data/ye/model_specific_' + key + '.pth'))
            print("✓ 加载特定疾病模型完成")
        else:
            
            y_train_labels = np.vstack(Y_train[dataset_key].values)
            num_count = []
            for idx in range(len(Y_train['comorbidity_status_multilabel'][0])):
                num_count.append(y_train_labels[:,idx].sum())

            sum_all = 0.0
            for idx in range(len(num_count)):
                num_count[idx] = 1.0/num_count[idx]
                sum_all += num_count[idx]

            for idx in range(len(num_count)):
                num_count[idx] = num_count[idx]/sum_all

            train_dataset = ProteinDatasetv2(X_train, Y_train, dataset_key)
            #train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
            true_labels = np.vstack(Y_test[dataset_key].values).astype(np.float32)
            model_specific = train_step_specific(model_specific, train_loader, 100, lr=0.0005, X_test=X_test, Y_test=true_labels, key=key, alpha=num_count)

        return_res[key] = {
                            "model": model,
                            "model_specific": model_specific,
                            "test_targets": Y_test,
                            "X_test": X_test
                        }
 
    return  return_res

# ========================== 结果分析函数 ==========================
def analyze_combination_performance(model_result, data):
    print("=== 分析疾病组合预测性能 ===")
    
    n_combinations = model_result["test_targets"].shape[1]
    combination_aucs = np.zeros(n_combinations)
    
    # 为每种组合计算AUC
    for i in range(n_combinations):
        y_true = model_result["test_targets"][:, i]
        y_pred = model_result["test_predictions"][:, i]
        
        if len(np.unique(y_true)) > 1 and np.sum(y_true) >= 5:
            try:
                auc_val = roc_auc_score(y_true, y_pred)
                combination_aucs[i] = auc_val
            except:
                combination_aucs[i] = 0.5
        else:
            combination_aucs[i] = np.nan
    
    # 创建结果表
    results_table = data["combination_info"].copy()
    results_table["auc"] = combination_aucs
    
    # 按AUC排序
    results_table = results_table.sort_values(by="auc", ascending=False)
    
    # 显示结果
    print("\n=== 疾病组合预测性能排行榜 ===")
    valid_results = results_table.dropna(subset=["auc"])
    
    if len(valid_results) > 0:
        print(f"有效预测的组合数: {len(valid_results)}/{n_combinations}")
        print(f"平均AUC: {valid_results['auc'].mean():.4f}")
        print(f"最佳AUC: {valid_results['auc'].max():.4f}")
        
        print("\n前15种表现最佳的疾病组合:")
        print(f"{'疾病组合':<40} {'GI':<10} {'Neuro':<10} {'样本数':<8} {'AUC':<8}")
        print("-" * 80)
        
        for i in range(min(15, len(valid_results))):
            row = valid_results.iloc[i]
            print(f"{row['combination_name'][:40]:<40} {row['gi_disease'][:10]:<10} {row['neuro_disease'][:10]:<10} {row['positive_cases']:<8} {row['auc']:.4f}")
    
    # 分析蛋白质重要性
    print("\n=== 最重要的蛋白质标记物 ===")
    
    # 计算平均蛋白质重要性
    avg_protein_importance = np.mean(model_result["protein_importance"], axis=0)
    protein_importance_dict = dict(zip(data["protein_names"], avg_protein_importance))
    
    # 排序并显示前20个最重要的蛋白质
    top_proteins = sorted(protein_importance_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    
    print("前20个最重要的蛋白质标记物:")
    for i, (protein, importance) in enumerate(top_proteins, 1):
        print(f"{i:2d}. {protein:<15}: 重要性 = {importance:.6f}")
    
    # 分析注意力权重
    avg_attention = np.mean(model_result["attention_weights"], axis=0)
    attention_dict = dict(zip(data["protein_names"], avg_attention))
    top_attention_proteins = sorted(attention_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    
    print("\n前20个最受关注的蛋白质（注意力机制）:")
    for i, (protein, weight) in enumerate(top_attention_proteins, 1):
        print(f"{i:2d}. {protein:<15}: 注意力权重 = {weight:.6f}")
    
    return {
        "combination_results": results_table,
        "top_proteins": [p[0] for p in top_proteins],
        "top_attention_proteins": [p[0] for p in top_attention_proteins],
        "protein_importance_scores": protein_importance_dict,
        "attention_weights": attention_dict
    }

# ========================== 核心蛋白质识别函数 ==========================
def calculate_single_combination_importance(model_result, full_data, combo_idx):
    y_true = model_result["test_targets"][:, combo_idx]
    y_pred = model_result["test_predictions"][:, combo_idx]
    X_test = model_result["X_test"]
    protein_names = full_data["protein_names"]
    
    protein_importance = np.zeros(len(protein_names))
    
    for j in range(len(protein_names)):
        protein_values = X_test[:, j]
        
        # 计算蛋白质与真实标签的相关性
        try:
            cor_true = np.abs(np.corrcoef(protein_values, y_true)[0, 1])
        except:
            cor_true = 0
        
        # 计算蛋白质与预测值的相关性
        try:
            cor_pred = np.abs(np.corrcoef(protein_values, y_pred)[0, 1])
        except:
            cor_pred = 0
        
        # 计算预测一致性
        median_val = np.median(protein_values)
        high_protein_indices = protein_values > median_val
        n_high = np.sum(high_protein_indices)
        
        if n_high > 5 and n_high < len(y_true):
            y_pred_binary = (y_pred > 0.5).astype(int)
            accuracy_high = np.mean(y_pred_binary[high_protein_indices] == y_true[high_protein_indices])
            accuracy_low = np.mean(y_pred_binary[~high_protein_indices] == y_true[~high_protein_indices])
            accuracy_diff = accuracy_high - accuracy_low
        else:
            accuracy_diff = 0
        
        # 整合评分
        protein_importance[j] = (cor_true * 0.4 + cor_pred * 0.3 + np.abs(accuracy_diff) * 0.3)
    
    return dict(zip(protein_names, protein_importance))

def analyze_top_auc_combinations(model_result, full_data):
    print("=== 为AUC表现最佳的疾病组合分析核心蛋白质 ===")
    
    n_combinations = model_result["test_targets"].shape[1]
    combination_aucs = np.zeros(n_combinations)
    valid_combinations = np.zeros(n_combinations, dtype=bool)
    
    for i in range(n_combinations):
        y_true = model_result["test_targets"][:, i]
        y_pred = model_result["test_predictions"][:, i]
        
        if len(np.unique(y_true)) > 1 and np.sum(y_true) >= 5:
            try:
                auc_val = roc_auc_score(y_true, y_pred)
                combination_aucs[i] = auc_val
                valid_combinations[i] = True
            except:
                combination_aucs[i] = 0.5
                valid_combinations[i] = False
        else:
            combination_aucs[i] = np.nan
            valid_combinations[i] = False
    
    # 创建AUC结果表
    auc_results = full_data["combination_info"].copy()
    auc_results["auc"] = combination_aucs
    auc_results["valid"] = valid_combinations
    
    # 只保留有效的组合并按AUC排序
    valid_auc_results = auc_results[auc_results["valid"]].dropna(subset=["auc"])
    valid_auc_results = valid_auc_results.sort_values(by="auc", ascending=False)
    
    print(f"有效组合数: {len(valid_auc_results)}")
    print("分析前15种AUC最佳组合的核心蛋白质...\n")
    
    # 为前15种AUC最佳组合分析核心蛋白质
    top15_combinations = valid_auc_results.head(15)
    top_combination_proteins = {}
    
    for i, (idx, row) in enumerate(top15_combinations.iterrows(), 1):
        combo_idx = idx
        combo_name = row["combination_name"]
        
        print(f"分析第{i}名: {combo_name} (AUC={row['auc']:.4f}, n={row['positive_cases']})")
        
        # 计算该组合的蛋白质重要性
        protein_importance = calculate_single_combination_importance(
            model_result, full_data, combo_idx
        )
        
        # 选择前10个重要蛋白质
        top_proteins = sorted(protein_importance.items(), key=lambda x: x[1], reverse=True)[:10]
        protein_names = [p[0] for p in top_proteins]
        protein_scores = [p[1] for p in top_proteins]
        
        top_combination_proteins[combo_name] = {
            "rank": i,
            "auc": row["auc"],
            "positive_cases": row["positive_cases"],
            "gi_disease": row["gi_disease"],
            "neuro_disease": row["neuro_disease"],
            "proteins": protein_names,
            "scores": protein_scores
        }
    
    return top_combination_proteins

def generate_top_auc_report(top_combination_proteins):
    print("\n=== AUC表现最佳疾病组合的核心蛋白质 ===")
    
    # 蛋白质功能数据库
    protein_functions = {
        "GDF15": "生长分化因子，炎症和代谢调节",
        "TNFRSF1A": "TNF受体，炎症信号传导",
        "FSTL3": "卵泡抑素，炎症调节",
        "CHI3L1": "几丁质酶样蛋白，炎症标记物",
        "IL18R1": "IL-18受体，炎症反应",
        "CD14": "内毒素受体，先天免疫",
        "HSPG2": "硫酸肝素蛋白聚糖，细胞外基质",
        "SELENOP": "硒蛋白P，抗氧化保护",
        "NUCB2": "核结合蛋白，神经内分泌调节",
        "ITGB2": "整合素β2，免疫细胞粘附",
        "FURIN": "前蛋白转化酶，蛋白质成熟",
        "LGALS9": "半乳糖凝集素9，免疫调节",
        "TNFRSF14": "TNF受体14，T细胞调节",
        "TGFBR2": "TGF-β受体2，细胞增殖和分化",
        "EFNA4": "ephrin A4，神经发育和血管生成",
        "SCARB2": "清道夫受体B2，脂质代谢",
        "IGFBP4": "IGF结合蛋白4，生长调节",
        "EDA2R": "ectodysplasin A2受体，发育调节",
        "TNFRSF1B": "TNF受体1B，细胞生存信号",
        "CST3": "胱抑素C，蛋白酶抑制剂",
        "EPS8L2": "表皮生长因子受体激酶底物8样蛋白2",
        "TFF2": "三叶因子2，胃肠道保护",
        "FABP4": "脂肪酸结合蛋白4，脂质代谢",
        "CTSZ": "组织蛋白酶Z，蛋白质降解",
        "FABP1": "脂肪酸结合蛋白1，肝脏脂质代谢",
        "CD74": "MHC II类分子伴侣，免疫反应",
        "TIMP1": "金属蛋白酶抑制剂1，组织重塑",
        "CSF1": "集落刺激因子1，巨噬细胞分化",
        "CKAP4": "细胞骨架相关蛋白4，细胞信号传导",
        "SPON2": "海绵蛋白2，细胞外基质",
        "TGFB1": "转化生长因子β1，细胞增殖和分化",
        "ASGR1": "去唾液酸糖蛋白受体1，肝脏清除功能",
        "SIAE": "唾液酸酯酶，糖蛋白代谢",
        "DPP7": "二肽基肽酶7，蛋白质处理",
        "CXCL11": "趋化因子配体11，免疫细胞招募",
        "SHISA5": "shisa家族蛋白5，神经发育",
        "B4GALT1": "β-1,4-半乳糖基转移酶1，糖蛋白合成",
        "RBFOX3": "RNA结合蛋白，神经元特异性",
        "PGF": "胎盘生长因子，血管生成",
        "VSIG4": "V型免疫球蛋白结构域蛋白4，免疫调节",
        "GLA": "α-半乳糖苷酶A，溶酶体酶"
    }
    
    detailed_results = []
    
    # 显示前10种最佳组合的详细信息
    top_combos = list(top_combination_proteins.keys())[:10]
    
    for combo_name in top_combos:
        combo_data = top_combination_proteins[combo_name]
        
        print(f"\n🏆 第{combo_data['rank']}名: {combo_name}")
        print(f"   AUC: {combo_data['auc']:.4f} | 样本数: {combo_data['positive_cases']}例 | GI: {combo_data['gi_disease']} | Neuro: {combo_data['neuro_disease']}")
        print("   核心蛋白质:")
        print("   排名 蛋白质         重要性   功能描述")
        print("   " + "-" * 70)
        
        for j, (protein, score) in enumerate(zip(combo_data["proteins"], combo_data["scores"]), 1):
            func_desc = protein_functions.get(protein, "功能待进一步研究")
            
            print(f"   {j:<4d} {protein:<15} {score:<8.4f} {func_desc[:45]}")
            
            detailed_results.append({
                "rank": combo_data["rank"],
                "combination_name": combo_name,
                "auc": combo_data["auc"],
                "positive_cases": combo_data["positive_cases"],
                "protein_rank": j,
                "protein_name": protein,
                "importance_score": score,
                "protein_function": func_desc
            })
    
    # 保存详细结果
    detailed_df = pd.DataFrame(detailed_results)
    os.makedirs("gi_neuro_complete_results", exist_ok=True)
    detailed_df.to_csv("gi_neuro_complete_results/top_auc_combination_proteins.csv", index=False)
    
    return detailed_df

def analyze_star_proteins_in_top_combinations(top_combination_proteins):
    print("\n=== 明星蛋白质在高AUC组合中的表现 ===")
    
    # 统计每个蛋白质在top组合中的出现频率
    all_top_proteins = []
    for combo in top_combination_proteins.values():
        all_top_proteins.extend(combo["proteins"])
    
    top_protein_frequency = pd.Series(all_top_proteins).value_counts().to_dict()
    
    # 计算每个蛋白质在top组合中的平均重要性
    protein_avg_importance = {}
    for protein in top_protein_frequency.keys():
        importance_scores = []
        for combo in top_combination_proteins.values():
            if protein in combo["proteins"]:
                idx = combo["proteins"].index(protein)
                importance_scores.append(combo["scores"][idx])
        protein_avg_importance[protein] = np.mean(importance_scores)
    
    # 创建综合评分表
    star_protein_analysis = []
    for protein, freq in top_protein_frequency.items():
        avg_imp = protein_avg_importance.get(protein, 0)
        composite_score = freq * avg_imp
        star_protein_analysis.append({
            "protein_name": protein,
            "frequency_in_top_combos": freq,
            "avg_importance": avg_imp,
            "composite_score": composite_score
        })
    
    star_df = pd.DataFrame(star_protein_analysis)
    star_df = star_df.sort_values("composite_score", ascending=False)
    
    print("🌟 在高AUC组合中表现最佳的蛋白质:")
    print(f"{'蛋白质':<15} {'出现次数':<8} {'平均重要性':<12} {'综合评分':<12}")
    print("-" * 55)
    
    for i, row in star_df.head(15).iterrows():
        print(f"{row['protein_name']:<15} {row['frequency_in_top_combos']:<8} {row['avg_importance']:<12.4f} {row['composite_score']:<12.4f}")
    
    # 保存结果
    star_df.to_csv("gi_neuro_complete_results/star_proteins_top_auc.csv", index=False)
    
    return star_df

# ========================== 主执行函数 ==========================
def main_multitask_analysis():
    print("\n" + "=" * 80)
    print("全蛋白质多任务深度神经网络分析")
    print("=" * 80)
    
    # 创建结果文件夹
    os.makedirs("gi_neuro_complete_results", exist_ok=True)
    
    try:
        # 步骤1：准备全蛋白质数据
        print("\n步骤1：准备数据...")
        full_data  = load_data("/data/ye/jiqi.csv")
        #full_data = prepare_full_protein_multitask_data(jiqi, protein_cols, gi_diseases, neuro_diseases)
        
        # 步骤2：训练多任务深度神经网络
        print("\n步骤2：训练多任务深度神经网络...")
        model_results = train_full_protein_multitask_model(
            data=full_data,
            pretrain_path="/data/ye/epoch_19_auc_1.4622.pth",
            epochs=100,
            lr=0.02,
            batch_size=256
        )
        


        for idx, key in enumerate(model_results):
            #key = specific_diseases[idx]
            model_result = model_results[key]
            # 步骤3：分析预测性能
            if model_result["model_specific"] is not None:
                model = model_result["model_specific"]
                X_test = model_result["X_test"]
                Y_test = model_result["test_targets"]
                print("\n进行测试集预测...")
                model.eval()
                pred_test = []
                heatmap = []

                with torch.no_grad():
                    # 分批处理测试数据避免内存问题
                    test_batch_size = 100
                    n_test = X_test.shape[0]
                    
                    for i in range(0, n_test, test_batch_size):
                        end_idx = min(i + test_batch_size, n_test)
                        X_batch = X_test[i:end_idx]
                        X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)
                        
                        outputs = model(X_batch_tensor)
                        
                        pred_test.append(outputs["combination_predictions"].cpu().numpy())
                        heatmap.append(outputs["protein_importance"].cpu().numpy())
                        #protein_importance.append(outputs["protein_importance"].cpu().numpy())

                pred_test = np.vstack(pred_test)
                protein_importance = np.vstack(heatmap)

                # remove no_protein
                protein_importance = protein_importance[:, 0:len(full_data['protein_cols'])]
                import shap
                import matplotlib.pyplot as plt
                plt.switch_backend('Agg')

                plt.rcParams["font.family"] = ["WenQuanYi Micro Hei"]
                # 解决负号显示问题
                plt.rcParams['axes.unicode_minus'] = False

                fig = shap.plots.violin((protein_importance-0.5)*2, show=False, feature_names=full_data['protein_cols'])  # show=False不显示图像，避免阻塞
                # 保存图片
                plt.savefig(key + '_shap_violin_plot.png', dpi=300, bbox_inches='tight')
                plt.close()  # 关闭图形，释放资源

                ### auc curves
                labels = np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32)
                if labels.ndim > 1 and labels.shape[1] > 1:
                    from sklearn.metrics import roc_curve, auc
                    # 多分类情况
                    fpr = dict()
                    tpr = dict()
                    roc_auc = dict()
                    n_classes = labels.shape[1]
                    
                    # 计算每个类别的ROC曲线和AUC
                    for i in range(n_classes):
                        fpr[i], tpr[i], _ = roc_curve(labels[:, i], pred_test[:, i])
                        roc_auc[i] = auc(fpr[i], tpr[i])
                    
                    # 绘制所有类别的ROC曲线
                    plt.figure(figsize=(10, 8))
                    for i in range(n_classes):
                        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
                    
                    plt.plot([0, 1], [0, 1], 'k--', lw=2)
                    plt.xlim([0.0, 1.0])
                    plt.ylim([0.0, 1.05])
                    plt.xlabel('False Positive Rate')
                    plt.ylabel('True Positive Rate')
                    plt.title('Receiver Operating Characteristic (ROC) Curves for Multi-class')
                    plt.legend(loc="lower right")
                    plt.savefig(key + '_roc_curves.png', dpi=300, bbox_inches='tight')
                    plt.close()  # 关闭图形，释放资源

                    # 计算并打印微平均AUC
                    fpr["micro"], tpr["micro"], _ = roc_curve(labels.ravel(), pred_test.ravel())
                    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
                    print(f"Micro-average AUC: {roc_auc['micro']:.4f}")

                # t-SNE scatter
                # 
                #     

                # 进行t-SNE降维并可视化
                from sklearn.manifold import TSNE
                import matplotlib.pyplot as plt
                import seaborn as sns

                print("进行t-SNE降维可视化...")

                # 如果特征维度很高，可以先使用PCA降维到50维再做t-SNE，加快速度
                from sklearn.decomposition import PCA
                pca = PCA(n_components=50)
                X_pca = pca.fit_transform(protein_importance)

                # 应用t-SNE
                tsne = TSNE(n_components=2, random_state=42, perplexity=30)
                X_tsne = tsne.fit_transform(X_pca)

                # 创建DataFrame用于绘图
                import pandas as pd
                tsne_df = pd.DataFrame({
                    'tsne_1': X_tsne[:, 0],
                    'tsne_2': X_tsne[:, 1],
                    'true_label': labels.argmax(axis=1) if labels.ndim > 1 else labels.flatten(),
                    # 取预测概率最大的类别作为预测标签
                    'pred_label': np.argmax(pred_test, axis=1) if pred_test.shape[1] > 1 else pred_test.flatten()
                })

                # 绘制t-SNE散点图（按真实标签着色）
                plt.figure(figsize=(12, 5))

                plt.subplot(1, 2, 1)
                sns.scatterplot(
                    x='tsne_1', y='tsne_2',
                    hue='true_label',
                    palette=sns.color_palette('hsv', n_colors=len(np.unique(labels.argmax(axis=1)))),
                    data=tsne_df,
                    alpha=0.7,
                    s=10
                )
                plt.title('t-SNE可视化 (按真实标签着色)')
                plt.xlabel('t-SNE维度1')
                plt.ylabel('t-SNE维度2')

                # 绘制t-SNE散点图（按预测标签着色）
                plt.subplot(1, 2, 2)
                sns.scatterplot(
                    x='tsne_1', y='tsne_2',
                    hue='pred_label',
                    palette=sns.color_palette('hsv', n_colors=len(np.unique(labels.argmax(axis=1)))),
                    data=tsne_df,
                    alpha=0.7,
                    s=10
                )
                plt.title('t-SNE可视化 (按预测标签着色)')
                plt.xlabel('t-SNE维度1')
                plt.ylabel('t-SNE维度2')

                plt.tight_layout()
                plt.savefig(key + '_tsne_visualization.png', dpi=300, bbox_inches='tight')
                plt.close()  # 关闭图形，释放资源

            else:
            

                #analysis_result = analyze_combination_performance(model_result, full_data)
                
                # 保存基本结果
                analysis_result["combination_results"].to_csv(
                    "gi_neuro_complete_results/all_combination_aucs.csv", index=False
                )
                
                # 步骤4：分析AUC最佳组合的核心蛋白质
                print("\n步骤4：分析AUC最佳组合的核心蛋白质...")
                top_auc_combination_proteins = analyze_top_auc_combinations(model_result, full_data)
                
                # 生成详细报告
                top_auc_detailed_results = generate_top_auc_report(top_auc_combination_proteins)
                
                # 分析明星蛋白质
                star_protein_analysis = analyze_star_proteins_in_top_combinations(top_auc_combination_proteins)
                
                # 保存最佳蛋白质
                if len(analysis_result["top_proteins"]) >= 10:
                    best_proteins = analysis_result["top_proteins"][:20]
                    best_proteins_df = pd.DataFrame({
                        "rank": range(1, 21),
                        "protein_name": best_proteins,
                        "importance_score": [analysis_result["protein_importance_scores"].get(p, 0) for p in best_proteins],
                        "attention_weight": [analysis_result["attention_weights"].get(p, 0) for p in best_proteins]
                    })
                    best_proteins_df.to_csv(
                        "gi_neuro_complete_results/best_proteins_multitask.csv", index=False
                    )
                
                print("\n✅ 全蛋白质多任务深度学习分析完成！")
                print("📊 结果已保存到 gi_neuro_complete_results/ 文件夹")
        
        else:
            print("\n⚠️ 预测阶段失败，但训练过程已完成")
    
    except Exception as e:
        print(f"❌ 执行失败: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("分析完成")
    print("=" * 60)

# ========================== 示例用法 ==========================
if __name__ == "__main__":
    # 执行分析
    #heatmap(np.random.rand(10, 12))  # 示例热力图

    from torchviz import make_dot
    import torch

    # 假设输入数据 x
    x = torch.randn(1, 200)  # batch_size=1, n_proteins=224

    # 初始化模型
    pretrained_model = PreTrainedProteinModel()
    pretrained_model.eval()
    specificity_model = SpecificityProteinNet(pretrained_model)
    specificity_model.eval()

    # 前向传播
    output = specificity_model(x)

    import matplotlib.pyplot as plt
    plt.switch_backend('Agg')
    # 绘制模型图
    dot = make_dot(output["combination_predictions"], params=dict(list(specificity_model.named_parameters())))
    dot.format = 'png'
    dot.render('SpecificityProteinNet')
    plt.savefig('model_structure.png', dpi=300, bbox_inches='tight')

    exit(-1)
    main_multitask_analysis()
