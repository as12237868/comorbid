import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')

# æ£€æŸ¥CUDAå¯ç”¨æ€§
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")


to_onehot = lambda x, length=4: [int(i == x) for i in range(length)]

specific_diseases = ["Stroke_status",  "Depression_status", "AD_status"]


def calculate_label(row, key):
    # æ¡ä»¶1: key_pd åŒæ—¶ has_gi ä¸º3
    if row[key] and row['has_gi'] :
        return 3
    # æ¡ä»¶2: åªæœ‰key_pd æ²¡æœ‰has_giä¸º2
    elif row[key] and not row['has_gi']:
        return 2
    # æ¡ä»¶3: has_gi æˆ–è€… has_neuro ä¸åŒ…å«has_pd ä¸º1
    elif row['has_gi'] or  row['has_neuro']:
        return 1
    # å…¶ä»–æƒ…å†µ
    else:
        return 0



def load_data(file_path):
    print("=== æ­¥éª¤1: æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç† ===")
    jiqi = pd.read_csv(file_path)

    # å®šä¹‰ç–¾ç—…å˜é‡
    gi_diseases = ["colorectal_status", "liver_status", "GERD_status", "IBD_status", 
                "IBS_status", "ALD_status", "NAFLD_status"]
    neuro_diseases = ["Stroke_status", "Anxiety_status", "Bipolar.affective.disorder_status", 
                    "Depression_status", "ACD_status", "AD_status", "VAD_status", 
                    "PD_status", "OACPD_status"]

    #specific_diseases = ["Stroke_status",  "Depression_status", "AD_status"]


    # è¿‡æ»¤å­˜åœ¨çš„åˆ—
    gi_diseases = [col for col in gi_diseases if col in jiqi.columns]
    neuro_diseases = [col for col in neuro_diseases if col in jiqi.columns]


    to_onehot_gi = lambda x:  np.array([0 for i in range(len(gi_diseases))])
    to_onehot_gi_2 = lambda x, idx=0:  np.array([int(i==idx) * x for i in range(len(gi_diseases))])
    jiqi['gi_label'] = jiqi[gi_diseases[0]].apply(to_onehot_gi)
    for idx, gi_diseas in enumerate(gi_diseases):
        jiqi['gi_label'] += jiqi[gi_diseas].apply(to_onehot_gi_2, idx=idx)

    to_onehot_neuro = lambda x:  np.array([0 for i in range(len(neuro_diseases))])
    to_onehot_neuro_2 = lambda x, idx=0:  np.array([int(i==idx) * x for i in range(len(neuro_diseases))])
    jiqi['neuro_label'] = jiqi[neuro_diseases[0]].apply(to_onehot_neuro)
    for idx, neuro_diseas in enumerate(neuro_diseases):
        jiqi['neuro_label'] += jiqi[neuro_diseas].apply(to_onehot_neuro_2, idx=idx)

    # åˆ›å»ºå…±ç—…æ ‡ç­¾
    jiqi['has_gi'] = jiqi[gi_diseases].apply(lambda x: any(x == 1), axis=1)
    jiqi['has_neuro'] = jiqi[neuro_diseases].apply(lambda x: any(x == 1), axis=1)
    jiqi['comorbidity_status'] = (jiqi['has_gi'] & jiqi['has_neuro']).astype(int)

    jiqi['comorbidity_status_multilabel'] = (jiqi['has_gi']).astype(int) + (jiqi['has_neuro']).astype(int) *2
    jiqi['comorbidity_status_multilabel'] = jiqi['comorbidity_status_multilabel'].apply(to_onehot)
    #jiqi['comorbidity_status_multilabel'] = jiqi['comorbidity_status'].apply(to_onehot,length=2)

    for key in specific_diseases:
        key_pd = 'has_' + key    
        jiqi[key_pd] = jiqi[key].apply(lambda x: int(x == 1))
        jiqi[key_pd + '_multilabel'] = jiqi.apply(calculate_label,axis=1, key=key_pd)
        jiqi[key_pd + '_multilabel'] = jiqi[key_pd + '_multilabel'].apply(to_onehot,length=4)
        #print(jiqi[key_pd + '_multilabel'].value_counts())


    for idx, val in enumerate(jiqi['gi_label'].mean() *100):
        item = gi_diseases[idx]
        print(f"gi_diseases {item} ç—…æ‚£ç—…ç‡: {val:.2f}%")

    for idx, val in enumerate(jiqi['neuro_label'].mean() *100):
        item = neuro_diseases[idx]
        print(f"ç¥ç»ç³»ç»Ÿ{item}ç—…æ‚£ç—…ç‡: {val:.2f}%")

    print(f"å…±ç—…æ ·æœ¬æ•°: {jiqi['comorbidity_status'].sum()}")
    print(f"å…±ç—…ç‡: {jiqi['comorbidity_status'].mean() * 100:.2f}%")

    # æå–è›‹ç™½è´¨ç‰¹å¾
    protein_cols = jiqi.loc[:, 'ADA2':'WARS'].columns.tolist()
    print(f"è›‹ç™½è´¨ç‰¹å¾æ•°é‡: {len(protein_cols)}")

    # å¤„ç†åå˜é‡
    covariates = ['age', 'TDI', 'BMI', 'Physical_activity']
    available_covariates = [col for col in covariates if col in jiqi.columns]

    # å¤„ç†Physical_activityåˆ—ä¸­çš„æ–‡æœ¬å€¼
    if 'Physical_activity' in jiqi.columns:
        # å°†æ–‡æœ¬å€¼æ˜ å°„ä¸ºæ•°å€¼
        activity_map = {'moderate': 1}
        jiqi['Physical_activity'] = jiqi['Physical_activity'].map(activity_map).fillna(jiqi['Physical_activity'])
        # ç¡®ä¿è¯¥åˆ—ä¸ºæ•°å€¼ç±»å‹
        jiqi['Physical_activity'] = pd.to_numeric(jiqi['Physical_activity'], errors='coerce').fillna(0)

    # å¤„ç†åˆ†ç±»å˜é‡
    if 'sex' in jiqi.columns:
        jiqi['sex_numeric'] = jiqi['sex'].map({'male': 1, 'female': 0}).fillna(0)
    else:
        jiqi['sex_numeric'] = 0

    categorical_cols = ['sex_numeric']
    if 'ethnic' in jiqi.columns:
        jiqi['ethnic_white'] = (jiqi['ethnic'] == 'White').astype(int)
        categorical_cols.append('ethnic_white')

    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    all_features = protein_cols + available_covariates + categorical_cols
    all_features = [col for col in all_features if col in jiqi.columns]

    X = jiqi[all_features].fillna(0).astype(float)
    #y = jiqi['comorbidity_status'].values

    # jiqi['x'] = X
    # jiqi['y'] = y

    print(f"æ€»ç‰¹å¾æ•°: {X.shape[1]}")

    label_cols = ['gi_label', 'neuro_label', 'comorbidity_status_multilabel']
    for key in specific_diseases:
        key_pd = 'has_' + key
        label_cols.append(key_pd + '_multilabel')
    return {
        "original_pd": jiqi,
        "X": X,
        "Y": jiqi[label_cols],
        "protein_cols": protein_cols
        #"Y": jiqi[['gi_label', 'neuro_label', 'comorbidity_status_multilabel']]
    }

# ========================== ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰ ==========================
class PreTrainedProteinModel(nn.Module):
    def __init__(self, n_proteins=200, n_gi=7, n_neuro=9, dropout_ratio=0.3):
        super(PreTrainedProteinModel, self).__init__()
        self.n_gi = n_gi
        self.n_neuro = n_neuro
        self.n_proteins = n_proteins
        self.feature_dim = 64
        
        # å…±äº«è›‹ç™½è´¨ç‰¹å¾æå–ç½‘ç»œ
        self.protein_encoder_gi = nn.Sequential(
            nn.Linear(n_proteins, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_ratio),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_ratio)
        )

        self.protein_encoder_neuro = nn.Sequential(
            nn.Linear(n_proteins, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_ratio),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_ratio)
        )

        
        # å¤šä»»åŠ¡è¾“å‡ºå¤´
        self.mutil_task_heads_gi = nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_gi)
            ) 
        

        self.mutil_task_heads_neuro =  nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_neuro)
            ) 
        

        
    def forward(self, x):
        gi_feature = self.protein_encoder_gi(x)
        neuro_feature = self.protein_encoder_neuro(x)
        gi_pred = self.mutil_task_heads_gi(gi_feature)
        neuro_pred = self.mutil_task_heads_neuro(neuro_feature)

        return {
            "gi_feature": gi_feature,
            "neuro_feature": neuro_feature,
            "gi_pred": gi_pred,
            "neuro_pred": neuro_pred
        }


class SpecificityProteinNet(nn.Module):
    def __init__(self, PreTrainModel, n_proteins=200, n_combin=4, dropout_ratio=0.3):
        super(SpecificityProteinNet, self).__init__()
        self.PreTrainModel = PreTrainModel
        self.n_combin = n_combin
        self.n_proteins = n_proteins
        self.dropout_ratio = dropout_ratio
        self.feature_dim = PreTrainModel.feature_dim

        self.input_attn_dim = 16 
        self.num_heads = 4
        self.head_dim = self.input_attn_dim * self.num_heads
        self.layer_norm = nn.LayerNorm(self.feature_dim)

        #cross attention module
   
        out_dim = n_proteins
        self.query = nn.Linear(self.feature_dim, self.head_dim)
        self.key = nn.Linear(self.feature_dim, self.head_dim)
        self.value = nn.Linear(self.feature_dim, self.feature_dim)

        # self.query = nn.Linear(self.input_attn_dim, self.input_attn_dim)
        # self.key = nn.Linear(self.input_attn_dim, self.input_attn_dim)
        # self.value = nn.Linear(self.input_attn_dim, self.input_attn_dim)


        self.out_proj = nn.Linear(self.feature_dim, out_dim)


        self.encoder = nn.Sequential(
            nn.Linear(n_proteins, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_ratio),
            nn.Linear(128, self.feature_dim ),
            nn.BatchNorm1d(self.feature_dim ),
            nn.ReLU(),
            nn.Dropout(dropout_ratio)
        )

        # ä»»åŠ¡ç‰¹å®šå¤´
        self.task_specific_heads = nn.Sequential(
                nn.Linear(self.feature_dim*3, 32),
                nn.BatchNorm1d(32),
                nn.ReLU(),
                nn.Dropout(dropout_ratio),
                nn.Linear(32, n_combin),
                nn.Softmax(dim=-1)
                #nn.ReLU()
            ) 
        

    def forward(self, x):
        with torch.no_grad():
            pretrain_out = self.PreTrainModel(x)
            gi_feature = pretrain_out["gi_feature"]
            neuro_feature = pretrain_out["neuro_feature"]


        q = self.query(gi_feature).reshape(-1, self.num_heads, self.input_attn_dim)  # [batch_size, seq_len, input_dim]
        k = self.key(neuro_feature).reshape(-1, self.num_heads, self.input_attn_dim)  # [batch_size, context_len, input_dim]
        v = self.value(neuro_feature).reshape(-1, self.num_heads, self.feature_dim//self.num_heads)  # [batch_size, context_len, input_dim]


        # q = self.query(gi_feature.reshape(-1, self.feature_dim//self.input_attn_dim,
        #             self.input_attn_dim))  # [batch_size, seq_len, input_dim]
        # k = self.key(neuro_feature.reshape(-1,self.feature_dim//self.input_attn_dim,
        #             self.input_attn_dim))  # [batch_size, context_len, input_dim]
        # v = self.value(neuro_feature.reshape(-1, self.feature_dim//self.input_attn_dim,
        #         self.input_attn_dim))  # [batch_size, context_len, input_dim]


        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        #attn_weights = torch.bmm(q, k.transpose(1, 2))  # [batch_size, seq_len, context_len]
        attn_weights = torch.bmm(q, k.transpose(1, 2)) / (self.input_attn_dim ** 0.5)  # [batch_size, seq_len, context_len]
        attn_weights = F.softmax(attn_weights, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›
        output = torch.bmm(attn_weights, v)  # [batch_size, seq_len, input_dim]
        output = output.reshape(-1, self.feature_dim)  # [batch_size, seq_len*input_dim]

        output = F.dropout(output, self.dropout_ratio, training=self.training)

        # æŠ•å½±åˆ°n-dim
        #output = self.layer_norm(output)
        mask = self.out_proj(self.layer_norm(output))  # [batch_size, seq_len, n_dim]
        mask =  torch.sigmoid(mask)

        x  = x * mask
        #x  = x * mask + x
        x = self.encoder(x)
        x = torch.cat([x, gi_feature, neuro_feature], dim=-1)

        x = F.dropout(x, self.dropout_ratio, training=self.training)

        x = self.task_specific_heads(x)
        return {
            "combination_predictions": x,
            "attention_weights": attn_weights,
            "protein_importance": mask
        }








# ========================== æ•°æ®å‡†å¤‡å‡½æ•° ==========================
def prepare_full_protein_multitask_data(jiqi, protein_cols, gi_diseases, neuro_diseases):
    print("=== å‡†å¤‡å…¨è›‹ç™½è´¨å¤šä»»åŠ¡æ•°æ® ===")
    
    # è·å–æ‰€æœ‰å¯ç”¨è›‹ç™½è´¨
    available_proteins = [col for col in protein_cols if col in jiqi.columns]
    print(f"æ€»è›‹ç™½è´¨æ•°é‡: {len(available_proteins)}")
    
    # æå–å…¨è›‹ç™½è´¨çŸ©é˜µ
    X_all_proteins = jiqi[available_proteins].copy()
    
    # æ•°æ®æ¸…ç†
    for col in X_all_proteins.columns:
        col_data = X_all_proteins[col]
        
        # ç¡®ä¿æ•°å€¼å‹
        if not np.issubdtype(col_data.dtype, np.number):
            X_all_proteins[col] = pd.to_numeric(col_data, errors='coerce')
        
        # å¤„ç†NAå€¼ï¼šç”¨ä¸­ä½æ•°å¡«å……
        if X_all_proteins[col].isna().any():
            median_val = X_all_proteins[col].median()
            if pd.isna(median_val):
                median_val = 0
            X_all_proteins[col].fillna(median_val, inplace=True)
    
    # æ ‡å‡†åŒ–æ‰€æœ‰è›‹ç™½è´¨
    scaler = StandardScaler()
    X_proteins_scaled = scaler.fit_transform(X_all_proteins)
    X_proteins_scaled = np.nan_to_num(X_proteins_scaled, nan=0, posinf=0, neginf=0)
    
    print(f"âœ“ è›‹ç™½è´¨ç‰¹å¾çŸ©é˜µ: {X_proteins_scaled.shape}")
    
    # å‡†å¤‡æ‰€æœ‰ç–¾ç—…ç»„åˆæ ‡ç­¾
    gi_diseases_available = [d for d in gi_diseases if d in jiqi.columns]
    neuro_diseases_available = [d for d in neuro_diseases if d in jiqi.columns]
    
    print(f"GIç–¾ç—…: {len(gi_diseases_available)} ç§")
    print(f"Neuroç–¾ç—…: {len(neuro_diseases_available)} ç§")
    
    # åˆ›å»ºæ‰€æœ‰ä¸¤ä¸¤ç»„åˆ
    n_combinations = len(gi_diseases_available) * len(neuro_diseases_available)
    combination_matrix = np.zeros((jiqi.shape[0], n_combinations))
    combination_info = {
        "gi_disease": [],
        "neuro_disease": [],
        "combination_name": [],
        "positive_cases": []
    }
    
    col_idx = 0
    for i in range(len(gi_diseases_available)):
        for j in range(len(neuro_diseases_available)):
            gi_disease = gi_diseases_available[i]
            neuro_disease = neuro_diseases_available[j]
            
            # æ£€æŸ¥åŒæ—¶æ‚£ç—…
            has_both = (jiqi[gi_disease] == 1) & (jiqi[neuro_disease] == 1)
            has_both.fillna(False, inplace=True)
            
            combination_matrix[:, col_idx] = has_both.astype(int)
            
            combination_info["gi_disease"].append(gi_disease)
            combination_info["neuro_disease"].append(neuro_disease)
            combination_info["combination_name"].append(f"{gi_disease}+{neuro_disease}")
            combination_info["positive_cases"].append(has_both.sum())
            
            col_idx += 1
    
    # è½¬æ¢ä¸ºDataFrame
    combination_info_df = pd.DataFrame(combination_info)
    
    # æ˜¾ç¤ºç»„åˆç»Ÿè®¡
    combination_info_df = combination_info_df.sort_values(by="positive_cases", ascending=False)
    print("\nå‰10ç§æœ€å¸¸è§ç»„åˆ:")
    for i in range(min(10, len(combination_info_df))):
        row = combination_info_df.iloc[i]
        print(f"{i+1}. {row['combination_name']}: {row['positive_cases']}ä¾‹")
    
    return {
        "X": X_proteins_scaled,
        "Y": combination_matrix,
        "combination_info": combination_info_df,
        "protein_names": available_proteins,
        "n_proteins": len(available_proteins),
        "n_combinations": n_combinations
    }

# ========================== æ•°æ®é›†ç±» ==========================
class ProteinDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.tensor(X, dtype=torch.float32)
        if type(Y) == pd.DataFrame: 
            self.Y_gi = torch.tensor(np.vstack(Y['gi_label'].values).astype(np.float32), dtype=torch.float32)
            self.Y_neuro = torch.tensor(np.vstack(Y['neuro_label'].values).astype(np.float32), dtype=torch.float32)
            self.Y_mutil = torch.tensor(np.vstack(Y['comorbidity_status_multilabel'].values).astype(np.float32), dtype=torch.float32)

    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y_gi[idx], self.Y_neuro[idx], self.Y_mutil[idx]

class ProteinDatasetv2(Dataset):
    def __init__(self, X, Y, key):
        self.X = torch.tensor(X, dtype=torch.float32)
        if type(Y) == pd.DataFrame and key in Y.columns: 
            self.Y = torch.tensor(np.vstack(Y[key].values).astype(np.float32), dtype=torch.float32)

    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y[idx]



def train_step_specific(model, train_loader, epochs, lr, X_test, Y_test, key, alpha):


    def eval_model(model, X_test, Y_test, epoch, best_auc, key):
        print("\nè¿›è¡Œæµ‹è¯•é›†é¢„æµ‹...")
        model.eval()
        pred_test = []
        heatmap = []

        with torch.no_grad():
            # åˆ†æ‰¹å¤„ç†æµ‹è¯•æ•°æ®é¿å…å†…å­˜é—®é¢˜
            test_batch_size = 100
            n_test = X_test.shape[0]
            
            for i in range(0, n_test, test_batch_size):
                end_idx = min(i + test_batch_size, n_test)
                X_batch = X_test[i:end_idx]
                X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)
                
                outputs = model(X_batch_tensor)
                
                pred_test.append(outputs["combination_predictions"].cpu().numpy())
                heatmap.append(outputs["protein_importance"].cpu().numpy())
                #protein_importance.append(outputs["protein_importance"].cpu().numpy())

        pred_test = np.vstack(pred_test)
        protein_importance = np.vstack(heatmap)


        #true_labels = np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1)
        true_labels = Y_test.argmax(axis=1)
        pred_labels = pred_test.argmax(axis=1)

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        for i in range(4):
            mask = true_labels == i
            if mask.any():
                acc = (pred_labels[mask] == i).mean()
            else:
                acc = 0.0
            print(f"Class_{i} ACC (epoch {epoch}): {acc:.4f}")

        # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
        overall_acc = (pred_labels == true_labels).mean()
        print(f"Overall ACC (epoch {epoch}): {overall_acc:.4f}")


        from sklearn.metrics import roc_auc_score
        #auc_sep = roc_auc_score(np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1).astype(np.int32),
   #         torch.tensor(pred_test).numpy(), multi_class='ovr', average=None)
        auc_sep = roc_auc_score(Y_test.argmax(axis=1).astype(np.int32), torch.tensor(pred_test).numpy(), multi_class='ovr', average=None)
        for i in range(4):
            print(f"Class_{i} AUC (epoch {epoch}): {auc_sep[i]:.4f}")

        # è®¡ç®—æ•´ä½“AUC
        # auc = roc_auc_score(np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1).astype(np.int32),
        #     torch.tensor(pred_test).numpy(), multi_class='ovr')
        auc = roc_auc_score(Y_test.argmax(axis=1).astype(np.int32), torch.tensor(pred_test).numpy(), multi_class='ovr')
        print(f"AUC_{epoch}: {auc:.4f}")

        if auc_sep[3] > best_auc:
            best_auc = auc_sep[3]
            #save the model
            torch.save(model.state_dict(), 'model_specific_' + key + '_' + str(epoch) + '_' + str(best_auc) + '.pth')

        return best_auc

    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params}")
    
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    from torch.optim.lr_scheduler import CosineAnnealingLR
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader), eta_min=0)

    # è®­ç»ƒå¾ªç¯
    train_losses = []
    print("\nå¼€å§‹è®­ç»ƒ...")


    class FocalLoss(nn.Module):
        def __init__(self, alpha=1, gamma=2, reduction='mean'):
            super(FocalLoss, self).__init__()
            self.alpha = alpha
            self.gamma = gamma
            self.reduction = reduction

        def forward(self, inputs, targets):
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
            
            # è·å–é¢„æµ‹æ¦‚ç‡
            pt = torch.exp(-ce_loss)
            
            # è®¡ç®—Focal Loss
            focal_loss = (1-pt)**self.gamma * ce_loss

            if self.alpha is not None:
                if isinstance(self.alpha, (float, int)):  # å¦‚æœæ˜¯æ ‡é‡ï¼Œç›´æ¥å¹¿æ’­
                    alpha_t = torch.full_like(targets, self.alpha, dtype=torch.float32)
                else:  # å¦‚æœæ˜¯å¼ é‡ [num_classes]ï¼ŒæŒ‰ targets ç´¢å¼•é€‰å– alpha
                    alpha_t = self.alpha[targets.argmax(1).cpu().numpy()].to(focal_loss.device)  # [batch_size]
                focal_loss = alpha_t * focal_loss  # æŒ‰æ ·æœ¬ç±»åˆ«åŠ æƒ

            if self.reduction == 'mean':
                return focal_loss.mean()
            elif self.reduction == 'sum':
                return focal_loss.sum()
            else:
                return focal_loss

            

        def auc_loss_for_class_3(self, inputs, targets):
            """ä¸“é—¨ä¼˜åŒ–å…±ç—…ç±»(Class 3)çš„AUC"""
            # å°†å››åˆ†ç±»è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼šClass 3 vs Others
            class_3_prob = F.softmax(inputs, dim=1)[:, 3]  # Class 3çš„æ¦‚ç‡
            is_class_3 = (targets.argmax(dim=1) == 3).float()  # æ˜¯å¦ä¸ºClass 3
            
            if is_class_3.sum() == 0 or (1 - is_class_3).sum() == 0:
                return torch.tensor(0.0, device=inputs.device)
            
            # è®¡ç®—AUCæŸå¤±
            pos_indices = (is_class_3 == 1).nonzero().squeeze()
            neg_indices = (is_class_3 == 0).nonzero().squeeze()
            
            if pos_indices.numel() == 0 or neg_indices.numel() == 0:
                return torch.tensor(0.0, device=inputs.device)
                
            pos_scores = class_3_prob[pos_indices]
            neg_scores = class_3_prob[neg_indices]
            
            # è®¡ç®—æ‰€æœ‰æ­£è´Ÿæ ·æœ¬å¯¹çš„å·®å€¼
            pos_scores = pos_scores.unsqueeze(1)  # [n_pos, 1]
            neg_scores = neg_scores.unsqueeze(0)  # [1, n_neg]
            
            # AUCæŸå¤±ï¼šæœ€å¤§åŒ– P(pos_score > neg_score)
            diff = pos_scores - neg_scores  # [n_pos, n_neg]
            auc_loss = torch.mean(torch.sigmoid(-diff))
            
            return auc_loss


    best_auc = 0 
    # alpha = torch.tensor([0.05, 0.05, 0.3, 0.6])  # æ€»å’Œä¸º1ï¼ˆå¯é€‰ï¼‰
    #alpha = torch.tensor([0.08, 0.25, 0.2, 0.47])  # æ€»å’Œä¸º1ï¼ˆå¯é€‰ï¼‰
    alpha = torch.tensor(alpha)
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        n_batches = 0
        
        for batch_x, batch_y  in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            #batch_y_gi = batch_y_gi.to(device)
            #batch_y_neuro = batch_y_neuro.to(device)

            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_x)            
            #"gi_pred": gi_pred,
            #"neuro_pred": neuro_pred

            # æˆ– alpha = torch.tensor([99, 1])   # æ€»å’Œä¸º100ï¼ˆåŒæ ·æœ‰æ•ˆï¼‰
            focal_loss = FocalLoss(alpha=alpha, gamma=2)
            loss = focal_loss(outputs['combination_predictions'], batch_y)

            #auc_loss = focal_loss.auc_loss_for_class_3(outputs['combination_predictions'], batch_y)
            #loss = 0.7 * loss + 0.3 * auc_loss 


            # è®¡ç®—æŸå¤±
            #loss = F.cross_entropy(outputs['combination_predictions'],batch_y)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if not torch.isnan(loss) and not torch.isinf(loss):
                # åå‘ä¼ æ’­
                loss.backward()
                scheduler.step()
                #print("LR =", scheduler.get_last_lr())
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                n_batches += 1
            else:
                print("è­¦å‘Šï¼šå‘ç°æ— æ•ˆæŸå¤±å€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
        
        if n_batches > 0:

            avg_loss = epoch_loss / n_batches
            train_losses.append(avg_loss)

            
            if (epoch + 1) % 1 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}")
                best_auc = eval_model(model, X_test, Y_test, epoch+1, best_auc, key)
        else:
            print(f"Epoch {epoch+1}/{epochs} - æ— æœ‰æ•ˆæ‰¹æ¬¡")
            break
    
    # æµ‹è¯•é›†é¢„æµ‹
    
    # heatmap_draw(protein_importance, np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32).argmax(axis=1).astype(np.int32))

    # visulazation the attention mask
    return  model



def heatmap_draw(data, label):
    import numpy as np
    import matplotlib
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    import seaborn as sns
    import matplotlib.pyplot as plt

        # å°†æ•°æ®å’Œæ ‡ç­¾è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(data)
    df['Label'] = label  # æ·»åŠ æ ‡ç­¾åˆ—
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„å¹¶è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼
    grouped = df.groupby('Label').mean()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    #data = np.random.rand(10, 12)  # 10è¡Œ12åˆ—çš„éšæœºæ•°æ®

    # åˆ›å»ºçƒ­åŠ›å›¾
    plt.figure(figsize=(10, 8))
    sns.heatmap(grouped, cmap="YlGnBu", fmt=".2f")


    plt.title("åŸºæœ¬çƒ­åŠ›å›¾ç¤ºä¾‹")
    plt.savefig("heatmap.png", dpi=300, bbox_inches="tight")

# ========================== è®­ç»ƒå‡½æ•° ==========================
def train_full_protein_multitask_model(data, pretrain_path= "", epochs=10, lr=0.001, batch_size=32):
    print("=== è®­ç»ƒå¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œ ===")

    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, Y_train, Y_test = train_test_split(
        data["X"].values.astype(np.float32), data["Y"], test_size=0.2, random_state=42
    )

    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    print(f"è›‹ç™½è´¨ç‰¹å¾: {X_train.shape[1]} ä¸ª")
    print(f"ç–¾ç—…ç»„åˆ: {Y_train.shape[1]} ä¸ª")

    batch_size = max(batch_size, 16)

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = ProteinDataset(X_train, Y_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # åˆå§‹åŒ–æ¨¡å‹
    model = PreTrainedProteinModel(
         n_proteins=X_train.shape[1],
         n_gi=Y_train['gi_label'][0].shape[0], 
         n_neuro=Y_train['neuro_label'][0].shape[0],
         dropout_ratio=0.5
    ).to(device)

    if os.path.exists(pretrain_path):
        print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrain_path}")
        model.load_state_dict(torch.load(pretrain_path, map_location=device))
    else:
        print("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        def eval_multi_task_model(model, X_test, Y_test, epoch, best_auc=0.0):
                
            # æµ‹è¯•é›†é¢„æµ‹
            print("\nè¿›è¡Œæµ‹è¯•é›†é¢„æµ‹...")
            model.eval()
            gi_pred_test = []
            neuro_pred_test = []

            
            with torch.no_grad():
                # åˆ†æ‰¹å¤„ç†æµ‹è¯•æ•°æ®é¿å…å†…å­˜é—®é¢˜
                test_batch_size = 100
                n_test = X_test.shape[0]
                
                for i in range(0, n_test, test_batch_size):
                    end_idx = min(i + test_batch_size, n_test)
                    X_batch = X_test[i:end_idx]
                    X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)
                    
                    outputs = model(X_batch_tensor)
                    
                    gi_pred_test.append(outputs["gi_pred"].cpu().numpy())
                    neuro_pred_test.append(outputs["neuro_pred"].cpu().numpy())
                    #protein_importance.append(outputs["protein_importance"].cpu().numpy())
            
            gi_pred_test = np.vstack(gi_pred_test)
            neuro_pred_test = np.vstack(neuro_pred_test)
            #protein_importance = np.vstack(protein_importance)

            ## count the acc of the predictions mutil label 
            def multi_label_auc(y_true, y_pred):

                from sklearn.metrics import roc_auc_score
                """
                è®¡ç®—å¤šæ ‡ç­¾AUCçš„å‡½æ•°
                :param y_true: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º[N, num_classes]
                :param y_pred: é¢„æµ‹æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º[N, num_classes]
                :return: å¤šæ ‡ç­¾AUC
                """
                # å°†æ ‡ç­¾è½¬æ¢ä¸ºnumpyæ•°ç»„
                #y_true = y_true.cpu().numpy()
                #y_pred = y_pred.cpu().numpy()

                # åˆå§‹åŒ–å¤šæ ‡ç­¾AUCå€¼
                total_auc = 0.

                # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„AUCå€¼ï¼Œå¹¶å¯¹æ‰€æœ‰æ ‡ç­¾çš„AUCå€¼æ±‚å¹³å‡
                for i in range(y_true.shape[1]):
                    try:
                        auc = roc_auc_score(y_true[:, i], y_pred[:, i])
                        print(f"Label {i} AUC: {auc:.4f}")
                    except ValueError:
                        auc = 0.5  # å¦‚æœæ ‡ç­¾ä¸­åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¿”å›0.5
                    total_auc += auc

                multi_auc = total_auc / y_true.shape[1]

                return multi_auc

            
            gi_auc = multi_label_auc(np.vstack(Y_test['gi_label'].values).astype(np.float32), gi_pred_test)
            neuro_auc = multi_label_auc(np.vstack(Y_test['neuro_label'].values).astype(np.float32), neuro_pred_test)

            #gi_acc = (gi_pred_test.argmax(axis=1) == Y_test['gi_label'].argmax(axis=1)).mean()
            #neuro_acc = (neuro_pred_test.argmax(axis=1) == Y_test['neuro_label'].argmax(axis=1)).mean()
            print(f"GI AUC: {gi_auc:.4f}")
            print(f"Neuro AUC: {neuro_auc:.4f}")
            if gi_auc + neuro_auc > best_auc:
                best_auc = gi_auc + neuro_auc
                print(f"ä¿å­˜æ¨¡å‹: epoch_{epoch}_auc_{best_auc:.4f}.pth")
                torch.save(model.state_dict(), f"epoch_{epoch}_auc_{best_auc:.4f}.pth")

            return best_auc
    
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params}")
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
        #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
            
            # æŸå¤±å‡½æ•°
        def multilabel_crossentropy(output,label):
            """
            å¤šæ ‡ç­¾åˆ†ç±»çš„äº¤å‰ç†µ
            è¯´æ˜ï¼šlabelå’Œoutputçš„shapeä¸€è‡´ï¼Œlabelçš„å…ƒç´ é0å³1ï¼Œ
                1è¡¨ç¤ºå¯¹åº”çš„ç±»ä¸ºç›®æ ‡ç±»ï¼Œ0è¡¨ç¤ºå¯¹åº”çš„ç±»ä¸ºéç›®æ ‡ç±»ã€‚
            è­¦å‘Šï¼šè¯·ä¿è¯outputçš„å€¼åŸŸæ˜¯å…¨ä½“å®æ•°ï¼Œæ¢è¨€ä¹‹ä¸€èˆ¬æƒ…å†µä¸‹output
                ä¸ç”¨åŠ æ¿€æ´»å‡½æ•°ï¼Œå°¤å…¶æ˜¯ä¸èƒ½åŠ sigmoidæˆ–è€…softmaxï¼é¢„æµ‹
                é˜¶æ®µåˆ™è¾“å‡ºoutputå¤§äº0çš„ç±»ã€‚å¦‚æœ‰ç–‘é—®ï¼Œè¯·ä»”ç»†é˜…è¯»å¹¶ç†è§£
                æœ¬æ–‡ã€‚
            :param output: [B,C]
            :param label:  [B,C]
            :return:
            """
            output = (1-2*label)*output
        
            #å¾—åˆ†å˜ä¸ºè´Ÿ1e12
            output_neg = output - label* 1e12
            output_pos = output-(1-label)* 1e12
        
            zeros = torch.zeros_like(output[:,:1])
        
            # [B, C + 1]
            output_neg = torch.cat([output_neg,zeros],dim=1)
            # [B, C + 1]
            output_pos = torch.cat([output_pos,zeros],dim=1)
        
            
            loss_pos = torch.logsumexp(output_pos,dim=1)
            loss_neg = torch.logsumexp(output_neg,dim=1)
            loss = (loss_neg + loss_pos).sum()
        
            return loss
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        print("\nå¼€å§‹è®­ç»ƒ...")
        
        best_auc = 0.0
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0.0
            n_batches = 0
            
            for batch_x, batch_y_gi, batch_y_neuro, _  in train_loader:
                batch_x = batch_x.to(device)
                batch_y_gi = batch_y_gi.to(device)
                batch_y_neuro = batch_y_neuro.to(device)

                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = model(batch_x)            
                #"gi_pred": gi_pred,
                #"neuro_pred": neuro_pred

                # è®¡ç®—æŸå¤±
                loss = multilabel_crossentropy(outputs["gi_pred"], batch_y_gi)
                loss += multilabel_crossentropy(outputs["neuro_pred"], batch_y_neuro)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if not torch.isnan(loss) and not torch.isinf(loss):
                    # åå‘ä¼ æ’­
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    n_batches += 1
                else:
                    print("è­¦å‘Šï¼šå‘ç°æ— æ•ˆæŸå¤±å€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            
            if n_batches > 0:
                scheduler.step()
                print("LR =", scheduler.get_last_lr())
                avg_loss = epoch_loss / n_batches / batch_size
                train_losses.append(avg_loss)
                
                if (epoch + 1) % 1 == 0:
                    print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}")
                    best_auc = eval_multi_task_model(model, X_test, Y_test, epoch+1, best_auc)
            else:
                print(f"Epoch {epoch+1}/{epochs} - æ— æœ‰æ•ˆæ‰¹æ¬¡")
                break

        #save the model 
        torch.save(model.state_dict(), 'model_pretrain.pth')

    #PreTraining model Traning finished 

    # training specific disease model

    print("\nå¼€å§‹è®­ç»ƒç–¾ç—…æ¨¡å‹...")

    return_res = {}
    for key in specific_diseases: 
        print(f"è®­ç»ƒç‰¹å®šç–¾ç—…æ¨¡å‹: {key}")
        dataset_key = 'has_' + key + '_multilabel'

    # åˆå§‹åŒ–æ¨¡å‹
        model_specific = SpecificityProteinNet(
            PreTrainModel= model,
            n_proteins=X_train.shape[1],
            n_combin=len(Y_train['comorbidity_status_multilabel'][0]),
            dropout_ratio=0.5
        ).to(device)

        if os.path.exists('/data/ye/model_specific_' + key + '.pth'):
            model_specific.load_state_dict(torch.load('/data/ye/model_specific_' + key + '.pth'))
            print("âœ“ åŠ è½½ç‰¹å®šç–¾ç—…æ¨¡å‹å®Œæˆ")
        else:
            
            y_train_labels = np.vstack(Y_train[dataset_key].values)
            num_count = []
            for idx in range(len(Y_train['comorbidity_status_multilabel'][0])):
                num_count.append(y_train_labels[:,idx].sum())

            sum_all = 0.0
            for idx in range(len(num_count)):
                num_count[idx] = 1.0/num_count[idx]
                sum_all += num_count[idx]

            for idx in range(len(num_count)):
                num_count[idx] = num_count[idx]/sum_all

            train_dataset = ProteinDatasetv2(X_train, Y_train, dataset_key)
            #train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
            true_labels = np.vstack(Y_test[dataset_key].values).astype(np.float32)
            model_specific = train_step_specific(model_specific, train_loader, 100, lr=0.0005, X_test=X_test, Y_test=true_labels, key=key, alpha=num_count)

        return_res[key] = {
                            "model": model,
                            "model_specific": model_specific,
                            "test_targets": Y_test,
                            "X_test": X_test
                        }
 
    return  return_res

# ========================== ç»“æœåˆ†æå‡½æ•° ==========================
def analyze_combination_performance(model_result, data):
    print("=== åˆ†æç–¾ç—…ç»„åˆé¢„æµ‹æ€§èƒ½ ===")
    
    n_combinations = model_result["test_targets"].shape[1]
    combination_aucs = np.zeros(n_combinations)
    
    # ä¸ºæ¯ç§ç»„åˆè®¡ç®—AUC
    for i in range(n_combinations):
        y_true = model_result["test_targets"][:, i]
        y_pred = model_result["test_predictions"][:, i]
        
        if len(np.unique(y_true)) > 1 and np.sum(y_true) >= 5:
            try:
                auc_val = roc_auc_score(y_true, y_pred)
                combination_aucs[i] = auc_val
            except:
                combination_aucs[i] = 0.5
        else:
            combination_aucs[i] = np.nan
    
    # åˆ›å»ºç»“æœè¡¨
    results_table = data["combination_info"].copy()
    results_table["auc"] = combination_aucs
    
    # æŒ‰AUCæ’åº
    results_table = results_table.sort_values(by="auc", ascending=False)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n=== ç–¾ç—…ç»„åˆé¢„æµ‹æ€§èƒ½æ’è¡Œæ¦œ ===")
    valid_results = results_table.dropna(subset=["auc"])
    
    if len(valid_results) > 0:
        print(f"æœ‰æ•ˆé¢„æµ‹çš„ç»„åˆæ•°: {len(valid_results)}/{n_combinations}")
        print(f"å¹³å‡AUC: {valid_results['auc'].mean():.4f}")
        print(f"æœ€ä½³AUC: {valid_results['auc'].max():.4f}")
        
        print("\nå‰15ç§è¡¨ç°æœ€ä½³çš„ç–¾ç—…ç»„åˆ:")
        print(f"{'ç–¾ç—…ç»„åˆ':<40} {'GI':<10} {'Neuro':<10} {'æ ·æœ¬æ•°':<8} {'AUC':<8}")
        print("-" * 80)
        
        for i in range(min(15, len(valid_results))):
            row = valid_results.iloc[i]
            print(f"{row['combination_name'][:40]:<40} {row['gi_disease'][:10]:<10} {row['neuro_disease'][:10]:<10} {row['positive_cases']:<8} {row['auc']:.4f}")
    
    # åˆ†æè›‹ç™½è´¨é‡è¦æ€§
    print("\n=== æœ€é‡è¦çš„è›‹ç™½è´¨æ ‡è®°ç‰© ===")
    
    # è®¡ç®—å¹³å‡è›‹ç™½è´¨é‡è¦æ€§
    avg_protein_importance = np.mean(model_result["protein_importance"], axis=0)
    protein_importance_dict = dict(zip(data["protein_names"], avg_protein_importance))
    
    # æ’åºå¹¶æ˜¾ç¤ºå‰20ä¸ªæœ€é‡è¦çš„è›‹ç™½è´¨
    top_proteins = sorted(protein_importance_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    
    print("å‰20ä¸ªæœ€é‡è¦çš„è›‹ç™½è´¨æ ‡è®°ç‰©:")
    for i, (protein, importance) in enumerate(top_proteins, 1):
        print(f"{i:2d}. {protein:<15}: é‡è¦æ€§ = {importance:.6f}")
    
    # åˆ†ææ³¨æ„åŠ›æƒé‡
    avg_attention = np.mean(model_result["attention_weights"], axis=0)
    attention_dict = dict(zip(data["protein_names"], avg_attention))
    top_attention_proteins = sorted(attention_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    
    print("\nå‰20ä¸ªæœ€å—å…³æ³¨çš„è›‹ç™½è´¨ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰:")
    for i, (protein, weight) in enumerate(top_attention_proteins, 1):
        print(f"{i:2d}. {protein:<15}: æ³¨æ„åŠ›æƒé‡ = {weight:.6f}")
    
    return {
        "combination_results": results_table,
        "top_proteins": [p[0] for p in top_proteins],
        "top_attention_proteins": [p[0] for p in top_attention_proteins],
        "protein_importance_scores": protein_importance_dict,
        "attention_weights": attention_dict
    }

# ========================== æ ¸å¿ƒè›‹ç™½è´¨è¯†åˆ«å‡½æ•° ==========================
def calculate_single_combination_importance(model_result, full_data, combo_idx):
    y_true = model_result["test_targets"][:, combo_idx]
    y_pred = model_result["test_predictions"][:, combo_idx]
    X_test = model_result["X_test"]
    protein_names = full_data["protein_names"]
    
    protein_importance = np.zeros(len(protein_names))
    
    for j in range(len(protein_names)):
        protein_values = X_test[:, j]
        
        # è®¡ç®—è›‹ç™½è´¨ä¸çœŸå®æ ‡ç­¾çš„ç›¸å…³æ€§
        try:
            cor_true = np.abs(np.corrcoef(protein_values, y_true)[0, 1])
        except:
            cor_true = 0
        
        # è®¡ç®—è›‹ç™½è´¨ä¸é¢„æµ‹å€¼çš„ç›¸å…³æ€§
        try:
            cor_pred = np.abs(np.corrcoef(protein_values, y_pred)[0, 1])
        except:
            cor_pred = 0
        
        # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
        median_val = np.median(protein_values)
        high_protein_indices = protein_values > median_val
        n_high = np.sum(high_protein_indices)
        
        if n_high > 5 and n_high < len(y_true):
            y_pred_binary = (y_pred > 0.5).astype(int)
            accuracy_high = np.mean(y_pred_binary[high_protein_indices] == y_true[high_protein_indices])
            accuracy_low = np.mean(y_pred_binary[~high_protein_indices] == y_true[~high_protein_indices])
            accuracy_diff = accuracy_high - accuracy_low
        else:
            accuracy_diff = 0
        
        # æ•´åˆè¯„åˆ†
        protein_importance[j] = (cor_true * 0.4 + cor_pred * 0.3 + np.abs(accuracy_diff) * 0.3)
    
    return dict(zip(protein_names, protein_importance))

def analyze_top_auc_combinations(model_result, full_data):
    print("=== ä¸ºAUCè¡¨ç°æœ€ä½³çš„ç–¾ç—…ç»„åˆåˆ†ææ ¸å¿ƒè›‹ç™½è´¨ ===")
    
    n_combinations = model_result["test_targets"].shape[1]
    combination_aucs = np.zeros(n_combinations)
    valid_combinations = np.zeros(n_combinations, dtype=bool)
    
    for i in range(n_combinations):
        y_true = model_result["test_targets"][:, i]
        y_pred = model_result["test_predictions"][:, i]
        
        if len(np.unique(y_true)) > 1 and np.sum(y_true) >= 5:
            try:
                auc_val = roc_auc_score(y_true, y_pred)
                combination_aucs[i] = auc_val
                valid_combinations[i] = True
            except:
                combination_aucs[i] = 0.5
                valid_combinations[i] = False
        else:
            combination_aucs[i] = np.nan
            valid_combinations[i] = False
    
    # åˆ›å»ºAUCç»“æœè¡¨
    auc_results = full_data["combination_info"].copy()
    auc_results["auc"] = combination_aucs
    auc_results["valid"] = valid_combinations
    
    # åªä¿ç•™æœ‰æ•ˆçš„ç»„åˆå¹¶æŒ‰AUCæ’åº
    valid_auc_results = auc_results[auc_results["valid"]].dropna(subset=["auc"])
    valid_auc_results = valid_auc_results.sort_values(by="auc", ascending=False)
    
    print(f"æœ‰æ•ˆç»„åˆæ•°: {len(valid_auc_results)}")
    print("åˆ†æå‰15ç§AUCæœ€ä½³ç»„åˆçš„æ ¸å¿ƒè›‹ç™½è´¨...\n")
    
    # ä¸ºå‰15ç§AUCæœ€ä½³ç»„åˆåˆ†ææ ¸å¿ƒè›‹ç™½è´¨
    top15_combinations = valid_auc_results.head(15)
    top_combination_proteins = {}
    
    for i, (idx, row) in enumerate(top15_combinations.iterrows(), 1):
        combo_idx = idx
        combo_name = row["combination_name"]
        
        print(f"åˆ†æç¬¬{i}å: {combo_name} (AUC={row['auc']:.4f}, n={row['positive_cases']})")
        
        # è®¡ç®—è¯¥ç»„åˆçš„è›‹ç™½è´¨é‡è¦æ€§
        protein_importance = calculate_single_combination_importance(
            model_result, full_data, combo_idx
        )
        
        # é€‰æ‹©å‰10ä¸ªé‡è¦è›‹ç™½è´¨
        top_proteins = sorted(protein_importance.items(), key=lambda x: x[1], reverse=True)[:10]
        protein_names = [p[0] for p in top_proteins]
        protein_scores = [p[1] for p in top_proteins]
        
        top_combination_proteins[combo_name] = {
            "rank": i,
            "auc": row["auc"],
            "positive_cases": row["positive_cases"],
            "gi_disease": row["gi_disease"],
            "neuro_disease": row["neuro_disease"],
            "proteins": protein_names,
            "scores": protein_scores
        }
    
    return top_combination_proteins

def generate_top_auc_report(top_combination_proteins):
    print("\n=== AUCè¡¨ç°æœ€ä½³ç–¾ç—…ç»„åˆçš„æ ¸å¿ƒè›‹ç™½è´¨ ===")
    
    # è›‹ç™½è´¨åŠŸèƒ½æ•°æ®åº“
    protein_functions = {
        "GDF15": "ç”Ÿé•¿åˆ†åŒ–å› å­ï¼Œç‚ç—‡å’Œä»£è°¢è°ƒèŠ‚",
        "TNFRSF1A": "TNFå—ä½“ï¼Œç‚ç—‡ä¿¡å·ä¼ å¯¼",
        "FSTL3": "åµæ³¡æŠ‘ç´ ï¼Œç‚ç—‡è°ƒèŠ‚",
        "CHI3L1": "å‡ ä¸è´¨é…¶æ ·è›‹ç™½ï¼Œç‚ç—‡æ ‡è®°ç‰©",
        "IL18R1": "IL-18å—ä½“ï¼Œç‚ç—‡ååº”",
        "CD14": "å†…æ¯’ç´ å—ä½“ï¼Œå…ˆå¤©å…ç–«",
        "HSPG2": "ç¡«é…¸è‚ç´ è›‹ç™½èšç³–ï¼Œç»†èƒå¤–åŸºè´¨",
        "SELENOP": "ç¡’è›‹ç™½Pï¼ŒæŠ—æ°§åŒ–ä¿æŠ¤",
        "NUCB2": "æ ¸ç»“åˆè›‹ç™½ï¼Œç¥ç»å†…åˆ†æ³Œè°ƒèŠ‚",
        "ITGB2": "æ•´åˆç´ Î²2ï¼Œå…ç–«ç»†èƒç²˜é™„",
        "FURIN": "å‰è›‹ç™½è½¬åŒ–é…¶ï¼Œè›‹ç™½è´¨æˆç†Ÿ",
        "LGALS9": "åŠä¹³ç³–å‡é›†ç´ 9ï¼Œå…ç–«è°ƒèŠ‚",
        "TNFRSF14": "TNFå—ä½“14ï¼ŒTç»†èƒè°ƒèŠ‚",
        "TGFBR2": "TGF-Î²å—ä½“2ï¼Œç»†èƒå¢æ®–å’Œåˆ†åŒ–",
        "EFNA4": "ephrin A4ï¼Œç¥ç»å‘è‚²å’Œè¡€ç®¡ç”Ÿæˆ",
        "SCARB2": "æ¸…é“å¤«å—ä½“B2ï¼Œè„‚è´¨ä»£è°¢",
        "IGFBP4": "IGFç»“åˆè›‹ç™½4ï¼Œç”Ÿé•¿è°ƒèŠ‚",
        "EDA2R": "ectodysplasin A2å—ä½“ï¼Œå‘è‚²è°ƒèŠ‚",
        "TNFRSF1B": "TNFå—ä½“1Bï¼Œç»†èƒç”Ÿå­˜ä¿¡å·",
        "CST3": "èƒ±æŠ‘ç´ Cï¼Œè›‹ç™½é…¶æŠ‘åˆ¶å‰‚",
        "EPS8L2": "è¡¨çš®ç”Ÿé•¿å› å­å—ä½“æ¿€é…¶åº•ç‰©8æ ·è›‹ç™½2",
        "TFF2": "ä¸‰å¶å› å­2ï¼Œèƒƒè‚ é“ä¿æŠ¤",
        "FABP4": "è„‚è‚ªé…¸ç»“åˆè›‹ç™½4ï¼Œè„‚è´¨ä»£è°¢",
        "CTSZ": "ç»„ç»‡è›‹ç™½é…¶Zï¼Œè›‹ç™½è´¨é™è§£",
        "FABP1": "è„‚è‚ªé…¸ç»“åˆè›‹ç™½1ï¼Œè‚è„è„‚è´¨ä»£è°¢",
        "CD74": "MHC IIç±»åˆ†å­ä¼´ä¾£ï¼Œå…ç–«ååº”",
        "TIMP1": "é‡‘å±è›‹ç™½é…¶æŠ‘åˆ¶å‰‚1ï¼Œç»„ç»‡é‡å¡‘",
        "CSF1": "é›†è½åˆºæ¿€å› å­1ï¼Œå·¨å™¬ç»†èƒåˆ†åŒ–",
        "CKAP4": "ç»†èƒéª¨æ¶ç›¸å…³è›‹ç™½4ï¼Œç»†èƒä¿¡å·ä¼ å¯¼",
        "SPON2": "æµ·ç»µè›‹ç™½2ï¼Œç»†èƒå¤–åŸºè´¨",
        "TGFB1": "è½¬åŒ–ç”Ÿé•¿å› å­Î²1ï¼Œç»†èƒå¢æ®–å’Œåˆ†åŒ–",
        "ASGR1": "å»å”¾æ¶²é…¸ç³–è›‹ç™½å—ä½“1ï¼Œè‚è„æ¸…é™¤åŠŸèƒ½",
        "SIAE": "å”¾æ¶²é…¸é…¯é…¶ï¼Œç³–è›‹ç™½ä»£è°¢",
        "DPP7": "äºŒè‚½åŸºè‚½é…¶7ï¼Œè›‹ç™½è´¨å¤„ç†",
        "CXCL11": "è¶‹åŒ–å› å­é…ä½“11ï¼Œå…ç–«ç»†èƒæ‹›å‹Ÿ",
        "SHISA5": "shisaå®¶æ—è›‹ç™½5ï¼Œç¥ç»å‘è‚²",
        "B4GALT1": "Î²-1,4-åŠä¹³ç³–åŸºè½¬ç§»é…¶1ï¼Œç³–è›‹ç™½åˆæˆ",
        "RBFOX3": "RNAç»“åˆè›‹ç™½ï¼Œç¥ç»å…ƒç‰¹å¼‚æ€§",
        "PGF": "èƒç›˜ç”Ÿé•¿å› å­ï¼Œè¡€ç®¡ç”Ÿæˆ",
        "VSIG4": "Vå‹å…ç–«çƒè›‹ç™½ç»“æ„åŸŸè›‹ç™½4ï¼Œå…ç–«è°ƒèŠ‚",
        "GLA": "Î±-åŠä¹³ç³–è‹·é…¶Aï¼Œæº¶é…¶ä½“é…¶"
    }
    
    detailed_results = []
    
    # æ˜¾ç¤ºå‰10ç§æœ€ä½³ç»„åˆçš„è¯¦ç»†ä¿¡æ¯
    top_combos = list(top_combination_proteins.keys())[:10]
    
    for combo_name in top_combos:
        combo_data = top_combination_proteins[combo_name]
        
        print(f"\nğŸ† ç¬¬{combo_data['rank']}å: {combo_name}")
        print(f"   AUC: {combo_data['auc']:.4f} | æ ·æœ¬æ•°: {combo_data['positive_cases']}ä¾‹ | GI: {combo_data['gi_disease']} | Neuro: {combo_data['neuro_disease']}")
        print("   æ ¸å¿ƒè›‹ç™½è´¨:")
        print("   æ’å è›‹ç™½è´¨         é‡è¦æ€§   åŠŸèƒ½æè¿°")
        print("   " + "-" * 70)
        
        for j, (protein, score) in enumerate(zip(combo_data["proteins"], combo_data["scores"]), 1):
            func_desc = protein_functions.get(protein, "åŠŸèƒ½å¾…è¿›ä¸€æ­¥ç ”ç©¶")
            
            print(f"   {j:<4d} {protein:<15} {score:<8.4f} {func_desc[:45]}")
            
            detailed_results.append({
                "rank": combo_data["rank"],
                "combination_name": combo_name,
                "auc": combo_data["auc"],
                "positive_cases": combo_data["positive_cases"],
                "protein_rank": j,
                "protein_name": protein,
                "importance_score": score,
                "protein_function": func_desc
            })
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_df = pd.DataFrame(detailed_results)
    os.makedirs("gi_neuro_complete_results", exist_ok=True)
    detailed_df.to_csv("gi_neuro_complete_results/top_auc_combination_proteins.csv", index=False)
    
    return detailed_df

def analyze_star_proteins_in_top_combinations(top_combination_proteins):
    print("\n=== æ˜æ˜Ÿè›‹ç™½è´¨åœ¨é«˜AUCç»„åˆä¸­çš„è¡¨ç° ===")
    
    # ç»Ÿè®¡æ¯ä¸ªè›‹ç™½è´¨åœ¨topç»„åˆä¸­çš„å‡ºç°é¢‘ç‡
    all_top_proteins = []
    for combo in top_combination_proteins.values():
        all_top_proteins.extend(combo["proteins"])
    
    top_protein_frequency = pd.Series(all_top_proteins).value_counts().to_dict()
    
    # è®¡ç®—æ¯ä¸ªè›‹ç™½è´¨åœ¨topç»„åˆä¸­çš„å¹³å‡é‡è¦æ€§
    protein_avg_importance = {}
    for protein in top_protein_frequency.keys():
        importance_scores = []
        for combo in top_combination_proteins.values():
            if protein in combo["proteins"]:
                idx = combo["proteins"].index(protein)
                importance_scores.append(combo["scores"][idx])
        protein_avg_importance[protein] = np.mean(importance_scores)
    
    # åˆ›å»ºç»¼åˆè¯„åˆ†è¡¨
    star_protein_analysis = []
    for protein, freq in top_protein_frequency.items():
        avg_imp = protein_avg_importance.get(protein, 0)
        composite_score = freq * avg_imp
        star_protein_analysis.append({
            "protein_name": protein,
            "frequency_in_top_combos": freq,
            "avg_importance": avg_imp,
            "composite_score": composite_score
        })
    
    star_df = pd.DataFrame(star_protein_analysis)
    star_df = star_df.sort_values("composite_score", ascending=False)
    
    print("ğŸŒŸ åœ¨é«˜AUCç»„åˆä¸­è¡¨ç°æœ€ä½³çš„è›‹ç™½è´¨:")
    print(f"{'è›‹ç™½è´¨':<15} {'å‡ºç°æ¬¡æ•°':<8} {'å¹³å‡é‡è¦æ€§':<12} {'ç»¼åˆè¯„åˆ†':<12}")
    print("-" * 55)
    
    for i, row in star_df.head(15).iterrows():
        print(f"{row['protein_name']:<15} {row['frequency_in_top_combos']:<8} {row['avg_importance']:<12.4f} {row['composite_score']:<12.4f}")
    
    # ä¿å­˜ç»“æœ
    star_df.to_csv("gi_neuro_complete_results/star_proteins_top_auc.csv", index=False)
    
    return star_df

# ========================== ä¸»æ‰§è¡Œå‡½æ•° ==========================
def main_multitask_analysis():
    print("\n" + "=" * 80)
    print("å…¨è›‹ç™½è´¨å¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œåˆ†æ")
    print("=" * 80)
    
    # åˆ›å»ºç»“æœæ–‡ä»¶å¤¹
    os.makedirs("gi_neuro_complete_results", exist_ok=True)
    
    try:
        # æ­¥éª¤1ï¼šå‡†å¤‡å…¨è›‹ç™½è´¨æ•°æ®
        print("\næ­¥éª¤1ï¼šå‡†å¤‡æ•°æ®...")
        full_data  = load_data("/data/ye/jiqi.csv")
        #full_data = prepare_full_protein_multitask_data(jiqi, protein_cols, gi_diseases, neuro_diseases)
        
        # æ­¥éª¤2ï¼šè®­ç»ƒå¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œ
        print("\næ­¥éª¤2ï¼šè®­ç»ƒå¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œ...")
        model_results = train_full_protein_multitask_model(
            data=full_data,
            pretrain_path="/data/ye/epoch_19_auc_1.4622.pth",
            epochs=100,
            lr=0.02,
            batch_size=256
        )
        


        for idx, key in enumerate(model_results):
            #key = specific_diseases[idx]
            model_result = model_results[key]
            # æ­¥éª¤3ï¼šåˆ†æé¢„æµ‹æ€§èƒ½
            if model_result["model_specific"] is not None:
                model = model_result["model_specific"]
                X_test = model_result["X_test"]
                Y_test = model_result["test_targets"]
                print("\nè¿›è¡Œæµ‹è¯•é›†é¢„æµ‹...")
                model.eval()
                pred_test = []
                heatmap = []

                with torch.no_grad():
                    # åˆ†æ‰¹å¤„ç†æµ‹è¯•æ•°æ®é¿å…å†…å­˜é—®é¢˜
                    test_batch_size = 100
                    n_test = X_test.shape[0]
                    
                    for i in range(0, n_test, test_batch_size):
                        end_idx = min(i + test_batch_size, n_test)
                        X_batch = X_test[i:end_idx]
                        X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)
                        
                        outputs = model(X_batch_tensor)
                        
                        pred_test.append(outputs["combination_predictions"].cpu().numpy())
                        heatmap.append(outputs["protein_importance"].cpu().numpy())
                        #protein_importance.append(outputs["protein_importance"].cpu().numpy())

                pred_test = np.vstack(pred_test)
                protein_importance = np.vstack(heatmap)

                # remove no_protein
                protein_importance = protein_importance[:, 0:len(full_data['protein_cols'])]
                import shap
                import matplotlib.pyplot as plt
                plt.switch_backend('Agg')

                plt.rcParams["font.family"] = ["WenQuanYi Micro Hei"]
                # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
                plt.rcParams['axes.unicode_minus'] = False

                fig = shap.plots.violin((protein_importance-0.5)*2, show=False, feature_names=full_data['protein_cols'])  # show=Falseä¸æ˜¾ç¤ºå›¾åƒï¼Œé¿å…é˜»å¡
                # ä¿å­˜å›¾ç‰‡
                plt.savefig(key + '_shap_violin_plot.png', dpi=300, bbox_inches='tight')
                plt.close()  # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾èµ„æº

                ### auc curves
                labels = np.vstack(Y_test['comorbidity_status_multilabel'].values).astype(np.float32)
                if labels.ndim > 1 and labels.shape[1] > 1:
                    from sklearn.metrics import roc_curve, auc
                    # å¤šåˆ†ç±»æƒ…å†µ
                    fpr = dict()
                    tpr = dict()
                    roc_auc = dict()
                    n_classes = labels.shape[1]
                    
                    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿å’ŒAUC
                    for i in range(n_classes):
                        fpr[i], tpr[i], _ = roc_curve(labels[:, i], pred_test[:, i])
                        roc_auc[i] = auc(fpr[i], tpr[i])
                    
                    # ç»˜åˆ¶æ‰€æœ‰ç±»åˆ«çš„ROCæ›²çº¿
                    plt.figure(figsize=(10, 8))
                    for i in range(n_classes):
                        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
                    
                    plt.plot([0, 1], [0, 1], 'k--', lw=2)
                    plt.xlim([0.0, 1.0])
                    plt.ylim([0.0, 1.05])
                    plt.xlabel('False Positive Rate')
                    plt.ylabel('True Positive Rate')
                    plt.title('Receiver Operating Characteristic (ROC) Curves for Multi-class')
                    plt.legend(loc="lower right")
                    plt.savefig(key + '_roc_curves.png', dpi=300, bbox_inches='tight')
                    plt.close()  # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾èµ„æº

                    # è®¡ç®—å¹¶æ‰“å°å¾®å¹³å‡AUC
                    fpr["micro"], tpr["micro"], _ = roc_curve(labels.ravel(), pred_test.ravel())
                    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
                    print(f"Micro-average AUC: {roc_auc['micro']:.4f}")

                # t-SNE scatter
                # 
                #     

                # è¿›è¡Œt-SNEé™ç»´å¹¶å¯è§†åŒ–
                from sklearn.manifold import TSNE
                import matplotlib.pyplot as plt
                import seaborn as sns

                print("è¿›è¡Œt-SNEé™ç»´å¯è§†åŒ–...")

                # å¦‚æœç‰¹å¾ç»´åº¦å¾ˆé«˜ï¼Œå¯ä»¥å…ˆä½¿ç”¨PCAé™ç»´åˆ°50ç»´å†åšt-SNEï¼ŒåŠ å¿«é€Ÿåº¦
                from sklearn.decomposition import PCA
                pca = PCA(n_components=50)
                X_pca = pca.fit_transform(protein_importance)

                # åº”ç”¨t-SNE
                tsne = TSNE(n_components=2, random_state=42, perplexity=30)
                X_tsne = tsne.fit_transform(X_pca)

                # åˆ›å»ºDataFrameç”¨äºç»˜å›¾
                import pandas as pd
                tsne_df = pd.DataFrame({
                    'tsne_1': X_tsne[:, 0],
                    'tsne_2': X_tsne[:, 1],
                    'true_label': labels.argmax(axis=1) if labels.ndim > 1 else labels.flatten(),
                    # å–é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹æ ‡ç­¾
                    'pred_label': np.argmax(pred_test, axis=1) if pred_test.shape[1] > 1 else pred_test.flatten()
                })

                # ç»˜åˆ¶t-SNEæ•£ç‚¹å›¾ï¼ˆæŒ‰çœŸå®æ ‡ç­¾ç€è‰²ï¼‰
                plt.figure(figsize=(12, 5))

                plt.subplot(1, 2, 1)
                sns.scatterplot(
                    x='tsne_1', y='tsne_2',
                    hue='true_label',
                    palette=sns.color_palette('hsv', n_colors=len(np.unique(labels.argmax(axis=1)))),
                    data=tsne_df,
                    alpha=0.7,
                    s=10
                )
                plt.title('t-SNEå¯è§†åŒ– (æŒ‰çœŸå®æ ‡ç­¾ç€è‰²)')
                plt.xlabel('t-SNEç»´åº¦1')
                plt.ylabel('t-SNEç»´åº¦2')

                # ç»˜åˆ¶t-SNEæ•£ç‚¹å›¾ï¼ˆæŒ‰é¢„æµ‹æ ‡ç­¾ç€è‰²ï¼‰
                plt.subplot(1, 2, 2)
                sns.scatterplot(
                    x='tsne_1', y='tsne_2',
                    hue='pred_label',
                    palette=sns.color_palette('hsv', n_colors=len(np.unique(labels.argmax(axis=1)))),
                    data=tsne_df,
                    alpha=0.7,
                    s=10
                )
                plt.title('t-SNEå¯è§†åŒ– (æŒ‰é¢„æµ‹æ ‡ç­¾ç€è‰²)')
                plt.xlabel('t-SNEç»´åº¦1')
                plt.ylabel('t-SNEç»´åº¦2')

                plt.tight_layout()
                plt.savefig(key + '_tsne_visualization.png', dpi=300, bbox_inches='tight')
                plt.close()  # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾èµ„æº

            else:
            

                #analysis_result = analyze_combination_performance(model_result, full_data)
                
                # ä¿å­˜åŸºæœ¬ç»“æœ
                analysis_result["combination_results"].to_csv(
                    "gi_neuro_complete_results/all_combination_aucs.csv", index=False
                )
                
                # æ­¥éª¤4ï¼šåˆ†æAUCæœ€ä½³ç»„åˆçš„æ ¸å¿ƒè›‹ç™½è´¨
                print("\næ­¥éª¤4ï¼šåˆ†æAUCæœ€ä½³ç»„åˆçš„æ ¸å¿ƒè›‹ç™½è´¨...")
                top_auc_combination_proteins = analyze_top_auc_combinations(model_result, full_data)
                
                # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
                top_auc_detailed_results = generate_top_auc_report(top_auc_combination_proteins)
                
                # åˆ†ææ˜æ˜Ÿè›‹ç™½è´¨
                star_protein_analysis = analyze_star_proteins_in_top_combinations(top_auc_combination_proteins)
                
                # ä¿å­˜æœ€ä½³è›‹ç™½è´¨
                if len(analysis_result["top_proteins"]) >= 10:
                    best_proteins = analysis_result["top_proteins"][:20]
                    best_proteins_df = pd.DataFrame({
                        "rank": range(1, 21),
                        "protein_name": best_proteins,
                        "importance_score": [analysis_result["protein_importance_scores"].get(p, 0) for p in best_proteins],
                        "attention_weight": [analysis_result["attention_weights"].get(p, 0) for p in best_proteins]
                    })
                    best_proteins_df.to_csv(
                        "gi_neuro_complete_results/best_proteins_multitask.csv", index=False
                    )
                
                print("\nâœ… å…¨è›‹ç™½è´¨å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ åˆ†æå®Œæˆï¼")
                print("ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° gi_neuro_complete_results/ æ–‡ä»¶å¤¹")
        
        else:
            print("\nâš ï¸ é¢„æµ‹é˜¶æ®µå¤±è´¥ï¼Œä½†è®­ç»ƒè¿‡ç¨‹å·²å®Œæˆ")
    
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆ")
    print("=" * 60)

# ========================== ç¤ºä¾‹ç”¨æ³• ==========================
if __name__ == "__main__":
    # æ‰§è¡Œåˆ†æ
    #heatmap(np.random.rand(10, 12))  # ç¤ºä¾‹çƒ­åŠ›å›¾

    from torchviz import make_dot
    import torch

    # å‡è®¾è¾“å…¥æ•°æ® x
    x = torch.randn(1, 200)  # batch_size=1, n_proteins=224

    # åˆå§‹åŒ–æ¨¡å‹
    pretrained_model = PreTrainedProteinModel()
    pretrained_model.eval()
    specificity_model = SpecificityProteinNet(pretrained_model)
    specificity_model.eval()

    # å‰å‘ä¼ æ’­
    output = specificity_model(x)

    import matplotlib.pyplot as plt
    plt.switch_backend('Agg')
    # ç»˜åˆ¶æ¨¡å‹å›¾
    dot = make_dot(output["combination_predictions"], params=dict(list(specificity_model.named_parameters())))
    dot.format = 'png'
    dot.render('SpecificityProteinNet')
    plt.savefig('model_structure.png', dpi=300, bbox_inches='tight')

    exit(-1)
    main_multitask_analysis()
